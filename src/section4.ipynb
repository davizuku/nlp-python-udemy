{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings and Word2Vec\n",
    "\n",
    "Word2Vec is an extension of the bigram model using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "\n",
    "CBOW - Continuous bag of words: Tries to predict a word given their surrounding words in a *Context*.\n",
    "\n",
    "1. Train a matrix $W_1$ with the embeddings of all words\n",
    "2. Given a set $C$ of all context words, take the average of all their embeddings: $h = \\sum_{c \\in C} W_1 c$\n",
    "3. Get probability of target word $y$: $p(y | C) = softmax(W_2^T h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram\n",
    "\n",
    "This is the opposite of CBOW: Try to predict surrounding words of a given word.\n",
    "1. Get embedding for $input$ word: $h = W_1 input$\n",
    "2. Get probability of target word $y$: $p(y | input) = softmax(W_2^T h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax complexity\n",
    "\n",
    "Vocabulary size $V$ can be huge (millions of words) and the model has to output a single one. \n",
    "\n",
    "Softmax cost of computation is $O(n)$, where $n$ is the size of the input vector, $V$ in our case. \n",
    "\n",
    "### Hierarchical softmax\n",
    "\n",
    "Consider the vocabulary as a binary tree where its leaves are words. \n",
    "\n",
    "At every node, calculate the probability: $p(go right) = \\sigma (v_{node}^T h)$. Decide whether to go left or right in the tree. \n",
    "\n",
    "At the end, probability $p(w | input) = \\prod_{node \\in path to w} \\sigma(v_{node}^T h_{input})$. \n",
    "It still satisfies the softmax rule: the sum fo all outputs is 1. \n",
    "\n",
    "No need for V x D matrix. Nodes only need to compute an scalar, so only a D vector is needed. \n",
    "\n",
    "Tree is built by putting frequent words at the top and uncommon words at the bottom, using Huffman coding. \n",
    "\n",
    "### Negative sampling\n",
    "\n",
    "Most of the time, a word is NOT the target. Instead of doing full multiclass cross-entropy, sample some wrong words and do binary cross-entropy.\n",
    "\n",
    "How many negative samples?\n",
    " - usually 5, 10 or even 25. \n",
    "\n",
    "How to choose negative samples?\n",
    " - modified unigram distribution\n",
    "     - unigram distribution is the probability of a single word occurring: $p(w) = \\frac{count(w)}{\\sum_{w'} count(w')}$\n",
    "     - *modified* because every count is modified by powering to $0.75$: \n",
    "     $$\\widetilde{p}(w) = \\frac{count(w)^{0.75}}{\\sum_{w'} count(w')^{0.75}}$$\n",
    "\n",
    "What if we sample a context word?\n",
    " - do not worry about this\n",
    " \n",
    "Negative samples can be chosen to replace:\n",
    " - context words\n",
    " - middle word\n",
    " \n",
    "\n",
    "\n",
    "Research has shown that Skip-gram with Negative sampling is what works the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which matrix should be used for word embeddings?\n",
    "\n",
    "In the word2vec model there are two matrices:\n",
    "- $W_1$: size V x D\n",
    "- $W_2$: size D x V \n",
    "\n",
    "Which of these contains the word embeddings?\n",
    "\n",
    "#### Option 1\n",
    "\n",
    "$W_e = W_1$\n",
    "\n",
    "#### Option 2\n",
    "\n",
    "$W_e = concat([W_1, W_2^T])$ which leads to shape V x 2D\n",
    "\n",
    "#### Option 3\n",
    "\n",
    "$W_e = (W_1 + W_2^T) / 2$\n",
    "\n",
    "Normalization is usually applied to vectors: \n",
    "\n",
    "$$\\hat{v} = \\frac{v}{|v|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec implementation tricks\n",
    "\n",
    "- Word frequencies follow a fat-tailed distribution. This leads to dropping a bunch of words. \n",
    "- Every time a sentence comes, some words are dropped according to a *modified uniform distribution* (see Negative sampling section).\n",
    "$$ p_{drop}(w) = 1 - \\sqrt{\\frac{threshold}{\\tilde{p}(w)}}$$\n",
    "  - Typical threshold: $10^{-5}$\n",
    "\n",
    "- Dropping words without any control, *effectively* widens the context window, since words that would not enter into the context, by dropping some words they now enter into it. E.g.: \n",
    "  - Sentence: \"The quick *brown fox* **jumps** *over the* lazy dog\"\n",
    "  - Drop: \"The\" and \"Over\"\n",
    "  - Sentence: \"Quick *brown fox* **jumps** *lazy dog*\"\n",
    "  - Words \"lazy\" and \"dog\" weren't in the context at the beginning\n",
    "\n",
    "- Learning rate scheduling: \n",
    "  - Set up a max and min learning rates\n",
    "  - Decrease learning rate during training\n",
    "  \n",
    "- Use parallel processings\n",
    "\n",
    "- Use C or Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec implementation outline\n",
    "\n",
    "- Load in data\n",
    "  - Turn each sentence into a list of integer word indexes\n",
    "  - Return a word to index mapping\n",
    "- Build the model\n",
    "  - Initialize parameters\n",
    "    - $W_1 = random(V, D)$\n",
    "    - $W_2 = random(D, V)$\n",
    "  - Calculate negative sampling distribution\n",
    "    - `p(w) = count(w) ^ 0.75 / sum(count(w) ^ 0.75)`\n",
    "  - Calculate subsampling distribution\n",
    "    - `p_drop(w) = 1 - sqrt(threshold / p(w))`\n",
    "- Train the model\n",
    "\n",
    "```\n",
    "# SGD - stochastic gradient descend\n",
    "for epoch in epochs:\n",
    "    for sentence in sentences:\n",
    "        sentence = subsample(sentence, p_drop)\n",
    "        for middle_word in sentence:\n",
    "            context = sentence[mid - window...mid + window]\n",
    "            SGD(middle_word, context, label-1)\n",
    "            neg_word = sample from p(w)\n",
    "            SGD(neg_word, context, label-0)\n",
    "            \n",
    "def SGD(word, context, label): \n",
    "    prob = sigmoid(W_1[word] . W_2[:, context])\n",
    "    gW_2 = outer(W_1[word], prob - label) # D x N\n",
    "    gW_1 = W_2[:, context] . (prob - label) # (D x N) (N) -> D\n",
    "    \n",
    "    W_2[:, context] -= lr * gW_2\n",
    "    W_1[word] -= lr * gW_1\n",
    "    \n",
    "    return -(label*log(prob) + (1-label)*log(1-prob)).sum()\n",
    "```\n",
    "\n",
    "- Test the trained model\n",
    "  - Print out word analogies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n",
      "total number of words in corpus: 39945445\n",
      "epoch complete: 0 cost: -94472020.9049495 dt: 1:12:26.694234\n",
      "processed 63600 / 576262\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/notebooks/vendor/machine_learning_examples/nlp_class2/word2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w2v_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m   \u001b[0;31m# word2idx, W, V = load_model('w2v_model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/vendor/machine_learning_examples/nlp_class2/word2vec.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(savedir)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# get the positive context words/negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mcontext_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mneg_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../vendor/machine_learning_examples'))\n",
    "%run -i '../vendor/machine_learning_examples/nlp_class2/word2vec.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in TF outline\n",
    "\n",
    "- No need to use SGD, but TF automatic differentiation\n",
    "- Making predictions: \n",
    "  - tf.nn.embedding_lookup(W, words)\n",
    "  - W = embedding\n",
    "  - words = word index list\n",
    "- To multiply two matrices: \n",
    "  - Input word: 1 x D matrix\n",
    "  - N context words: N x D matrix\n",
    "\n",
    "```\n",
    "def dot(A, B):\n",
    "    C = A * B\n",
    "    return tf.reduce_sum(C, axis=1)\n",
    "```\n",
    "\n",
    "- Getting the cost\n",
    "\n",
    "```\n",
    "correct_output = dot(pos_input, emb_output)\n",
    "pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=ones, \n",
    "    logits=correct_output\n",
    ")\n",
    "\n",
    "incorrect_output = dot(neg_input, emb_output)\n",
    "neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    lables=zeros,\n",
    "    logits=incorrect_output\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(pos_loss) + tf.reduce_mean(neg_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately these work different ways\n",
    "def remove_punctuation_2(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def remove_punctuation_3(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "if sys.version.startswith('2'):\n",
    "    remove_punctuation = remove_punctuation_2\n",
    "else:\n",
    "    remove_punctuation = remove_punctuation_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "    V = 20000\n",
    "    files = glob('../large_files/enwiki*.txt')\n",
    "    all_word_counts = {}\n",
    "    for f in files:\n",
    "        for line in open(f):\n",
    "            if line and line[0] not in '[*-|=\\{\\}':\n",
    "                s = remove_punctuation(line).lower().split()\n",
    "                if len(s) > 1:\n",
    "                    for word in s:\n",
    "                        if word not in all_word_counts:\n",
    "                            all_word_counts[word] = 0\n",
    "                        all_word_counts[word] += 1\n",
    "    print(\"finished counting\")\n",
    "\n",
    "    V = min(V, len(all_word_counts))\n",
    "    all_word_counts = sorted(\n",
    "        all_word_counts.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "    word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "    unk = word2idx['<UNK>']\n",
    "\n",
    "    sents = []\n",
    "    for f in files:\n",
    "        for line in open(f):\n",
    "            if line and line[0] not in '[*-|=\\{\\}':\n",
    "                s = remove_punctuation(line).lower().split()\n",
    "                if len(s) > 1:\n",
    "                    # if a word is not nearby another word, \n",
    "                    # there won't be any context!\n",
    "                    # and hence nothing to train!\n",
    "                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "                    sents.append(sent)\n",
    "    return sents, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(A, B):\n",
    "    C = A * B\n",
    "    return tf.reduce_sum(C, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences, vocab_size):\n",
    "    # Pn(w) = prob of word occuring\n",
    "    # we would like to sample the negative samples\n",
    "    # such that words that occur more often\n",
    "    # should be sampled more often\n",
    "\n",
    "    word_freq = np.zeros(vocab_size)\n",
    "    word_count = sum(len(sentence) for sentence in sentences)\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "    # smooth it\n",
    "    p_neg = word_freq**0.75\n",
    "\n",
    "    # normalize it\n",
    "    p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "    assert(np.all(p_neg > 0))\n",
    "    return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "    # input:\n",
    "    # a sentence of the form: x x x x c c c pos c c c x x x x\n",
    "    # output:\n",
    "    # the context word indices: c c c c c c\n",
    "\n",
    "    start = max(0, pos - window_size)\n",
    "    end_    = min(len(sentence), pos + window_size)\n",
    "\n",
    "    context = []\n",
    "    for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "        if ctx_pos != pos:\n",
    "            # don't include the input word itself as a target\n",
    "            context.append(ctx_word_idx)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution for drawing negative samples\n",
    "p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
    "\n",
    "# for subsampling each sentence\n",
    "threshold = 1e-5\n",
    "p_drop = 1 - np.sqrt(threshold / p_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# config\n",
    "window_size = 5\n",
    "learning_rate = 0.025\n",
    "final_learning_rate = 0.0001\n",
    "num_negatives = 5 # number of negative samples to draw per input word\n",
    "epochs = 5\n",
    "D = 50 # word embedding size\n",
    "\n",
    "# learning rate decay\n",
    "learning_rate_delta = (learning_rate - final_learning_rate) / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build model\n",
    "pos_input = tf.placeholder(tf.int32, shape=(None,), name='pos_input')\n",
    "neg_input = tf.placeholder(tf.int32, shape=(None,), name='neg_input')\n",
    "context_input = tf.placeholder(tf.int32, shape=(None,), name='context_input')\n",
    "\n",
    "W = np.random.randn(vocab_size, D).astype(np.float32) # input-to-hidden\n",
    "V = np.random.randn(D, vocab_size).astype(np.float32) # hidden-to-output\n",
    "\n",
    "W1 = tf.Variable(W)\n",
    "W2 = tf.Variable(V.T)\n",
    "\n",
    "emb_pos_input = tf.nn.embedding_lookup(W1, pos_input) # 1 x D\n",
    "emb_neg_input = tf.nn.embedding_lookup(W1, neg_input) # 1 x D\n",
    "emb_output = tf.nn.embedding_lookup(W2, context_input) # N x D\n",
    "\n",
    "correct_output = dot(emb_neg_input, emb_output)\n",
    "pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones(tf.shape(correct_output)), \n",
    "    logits=correct_output\n",
    ")\n",
    "incorrect_output = dot(emb_neg_input, emb_output)\n",
    "neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.zeros(tf.shape(incorrect_output)),\n",
    "    logits=incorrect_output\n",
    ")\n",
    "loss = tf.reduce_mean(pos_loss) + tf.reduce_mean(neg_loss)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5\n",
      "Error loss at 100/10000: 5.6774364\n",
      "Error loss at 200/10000: 5.928441\n",
      "Error loss at 300/10000: 5.7628384\n",
      "Error loss at 400/10000: 4.1086893\n",
      "Error loss at 500/10000: 7.0824327\n",
      "Error loss at 600/10000: 3.5298786\n",
      "Error loss at 700/10000: 4.2213736\n",
      "Error loss at 800/10000: 4.6359286\n",
      "Error loss at 900/10000: 7.1032376\n",
      "Error loss at 1000/10000: 5.1425514\n",
      "Error loss at 1100/10000: 4.6034107\n",
      "Error loss at 1200/10000: 5.4119906\n",
      "Error loss at 1300/10000: 5.021418\n",
      "Error loss at 1400/10000: 2.848061\n",
      "Error loss at 1500/10000: 5.489454\n",
      "Error loss at 1600/10000: 3.128865\n",
      "Error loss at 1700/10000: 1.9177787\n",
      "Error loss at 1800/10000: 4.3294587\n",
      "Error loss at 1900/10000: 9.354001\n",
      "Error loss at 2000/10000: 1.6874799\n",
      "Error loss at 2100/10000: 4.364646\n",
      "Error loss at 2200/10000: 6.0175037\n",
      "Error loss at 2300/10000: 8.166868\n",
      "Error loss at 2400/10000: 6.411989\n",
      "Error loss at 2500/10000: 6.1909585\n",
      "Error loss at 2600/10000: 4.742512\n",
      "Error loss at 2700/10000: 9.804838\n",
      "Error loss at 2800/10000: 8.723166\n",
      "Error loss at 2900/10000: 4.0937915\n",
      "Error loss at 3000/10000: 6.1574206\n",
      "Error loss at 3100/10000: 5.3736687\n",
      "Error loss at 3200/10000: 3.792172\n",
      "Error loss at 3300/10000: 3.5778956\n",
      "Error loss at 3400/10000: 5.193759\n",
      "Error loss at 3500/10000: 3.558599\n",
      "Error loss at 3600/10000: 4.508933\n",
      "Error loss at 3700/10000: 4.170625\n",
      "Error loss at 3800/10000: 5.2629604\n",
      "Error loss at 3900/10000: 6.847009\n",
      "Error loss at 4000/10000: 5.1532364\n",
      "Error loss at 4100/10000: 5.952021\n",
      "Error loss at 4200/10000: 7.539094\n",
      "Error loss at 4300/10000: 3.951633\n",
      "Error loss at 4400/10000: 1.3867644\n",
      "Error loss at 4500/10000: 5.5163403\n",
      "Error loss at 4600/10000: 4.9931374\n",
      "Error loss at 4700/10000: 7.070756\n",
      "Error loss at 4800/10000: 1.5951298\n",
      "Error loss at 4900/10000: 2.943979\n",
      "Error loss at 5000/10000: 4.266511\n",
      "Error loss at 5100/10000: 4.6495304\n",
      "Error loss at 5200/10000: 5.835226\n",
      "Error loss at 5300/10000: 2.944192\n",
      "Error loss at 5400/10000: 1.3862944\n",
      "Error loss at 5500/10000: 4.419503\n",
      "Error loss at 5600/10000: 6.5396366\n",
      "Error loss at 5700/10000: 4.4826345\n",
      "Error loss at 5800/10000: 4.285669\n",
      "Error loss at 5900/10000: 6.0766826\n",
      "Error loss at 6000/10000: 6.1806946\n",
      "Error loss at 6100/10000: 4.377873\n",
      "Error loss at 6200/10000: 3.6468227\n",
      "Error loss at 6300/10000: 5.9991875\n",
      "Error loss at 6400/10000: 4.5971184\n",
      "Error loss at 6500/10000: 1.3862944\n",
      "Error loss at 6600/10000: 7.116773\n",
      "Error loss at 6700/10000: 6.830734\n",
      "Error loss at 6800/10000: 3.5135446\n",
      "Error loss at 6900/10000: 3.6781194\n",
      "Error loss at 7000/10000: 3.7258444\n",
      "Error loss at 7100/10000: 5.690805\n",
      "Error loss at 7200/10000: 6.621798\n",
      "Error loss at 7300/10000: 4.863656\n",
      "Error loss at 7400/10000: 7.2998476\n",
      "Error loss at 7500/10000: 4.4093404\n",
      "Error loss at 7600/10000: 5.522657\n",
      "Error loss at 7700/10000: 7.5143137\n",
      "Error loss at 7800/10000: 5.2720327\n",
      "Error loss at 7900/10000: 1.4245178\n",
      "Error loss at 8000/10000: 6.6735516\n",
      "Error loss at 8100/10000: 4.3525233\n",
      "Error loss at 8200/10000: 2.7397077\n",
      "Error loss at 8300/10000: 4.6229997\n",
      "Error loss at 8400/10000: 1.3863108\n",
      "Error loss at 8500/10000: 6.071198\n",
      "Error loss at 8600/10000: 7.7680354\n",
      "Error loss at 8700/10000: 6.8831015\n",
      "Error loss at 8800/10000: 3.395896\n",
      "Error loss at 8900/10000: 3.3375013\n",
      "Error loss at 9000/10000: 6.671738\n",
      "Error loss at 9100/10000: 6.2263994\n",
      "Error loss at 9200/10000: 3.8064342\n",
      "Error loss at 9300/10000: 6.0454154\n",
      "Epoch 1 / 5\n",
      "Error loss at 9400/10000: 3.980971\n",
      "Error loss at 9500/10000: 9.253649\n",
      "Error loss at 9600/10000: 2.2518854\n",
      "Error loss at 9700/10000: 5.697474\n",
      "Error loss at 9800/10000: 2.25152\n",
      "Error loss at 9900/10000: 3.812509\n",
      "Error loss at 10000/10000: 1.9557838\n",
      "Error loss at 10100/10000: 3.0069647\n",
      "Error loss at 10200/10000: 4.4624224\n",
      "Error loss at 10300/10000: 3.842557\n",
      "Error loss at 10400/10000: 3.7089083\n",
      "Error loss at 10500/10000: 4.4382873\n",
      "Error loss at 10600/10000: 5.9112797\n",
      "Error loss at 10700/10000: 4.393855\n",
      "Error loss at 10800/10000: 4.674031\n",
      "Error loss at 10900/10000: 3.608502\n",
      "Error loss at 11000/10000: 1.3870239\n",
      "Error loss at 11100/10000: 4.7759504\n",
      "Error loss at 11200/10000: 6.306893\n",
      "Error loss at 11300/10000: 1.3923008\n",
      "Error loss at 11400/10000: 5.824485\n",
      "Error loss at 11500/10000: 1.3862944\n",
      "Error loss at 11600/10000: 3.5137157\n",
      "Error loss at 11700/10000: 5.205576\n",
      "Error loss at 11800/10000: 4.5783753\n",
      "Error loss at 11900/10000: 3.1802688\n",
      "Error loss at 12000/10000: 2.5238264\n",
      "Error loss at 12100/10000: 4.984978\n",
      "Error loss at 12200/10000: 2.5440872\n",
      "Error loss at 12300/10000: 1.3872046\n",
      "Error loss at 12400/10000: 1.3892022\n",
      "Error loss at 12500/10000: 5.4769974\n",
      "Error loss at 12600/10000: 1.3862944\n",
      "Error loss at 12700/10000: 6.4663305\n",
      "Error loss at 12800/10000: 7.424568\n",
      "Error loss at 12900/10000: 4.843882\n",
      "Error loss at 13000/10000: 2.4397597\n",
      "Error loss at 13100/10000: 5.2527504\n",
      "Error loss at 13200/10000: 6.6460857\n",
      "Error loss at 13300/10000: 3.1562424\n",
      "Error loss at 13400/10000: 4.484294\n",
      "Error loss at 13500/10000: 5.720749\n",
      "Error loss at 13600/10000: 1.3862944\n",
      "Error loss at 13700/10000: 6.202173\n",
      "Error loss at 13800/10000: 4.169986\n",
      "Error loss at 13900/10000: 5.4526\n",
      "Error loss at 14000/10000: 1.7642244\n",
      "Error loss at 14100/10000: 6.9221306\n",
      "Error loss at 14200/10000: 5.9486837\n",
      "Error loss at 14300/10000: 2.780645\n",
      "Error loss at 14400/10000: 3.3731728\n",
      "Error loss at 14500/10000: 4.5335197\n",
      "Error loss at 14600/10000: 3.341883\n",
      "Error loss at 14700/10000: 6.617564\n",
      "Error loss at 14800/10000: 6.3338823\n",
      "Error loss at 14900/10000: 7.4918485\n",
      "Error loss at 15000/10000: 8.633778\n",
      "Error loss at 15100/10000: 1.3863325\n",
      "Error loss at 15200/10000: 5.9906554\n",
      "Error loss at 15300/10000: 4.9613743\n",
      "Error loss at 15400/10000: 1.3863897\n",
      "Error loss at 15500/10000: 2.9918246\n",
      "Error loss at 15600/10000: 1.3863325\n",
      "Error loss at 15700/10000: 1.4631788\n",
      "Error loss at 15800/10000: 4.2018414\n",
      "Error loss at 15900/10000: 3.6675224\n",
      "Error loss at 16000/10000: 2.7799978\n",
      "Error loss at 16100/10000: 8.20302\n",
      "Error loss at 16200/10000: 3.253367\n",
      "Error loss at 16300/10000: 5.0163164\n",
      "Error loss at 16400/10000: 4.4243393\n",
      "Error loss at 16500/10000: 4.7668695\n",
      "Error loss at 16600/10000: 4.5772104\n",
      "Error loss at 16700/10000: 5.303606\n",
      "Error loss at 16800/10000: 4.6068583\n",
      "Error loss at 16900/10000: 1.3864267\n",
      "Error loss at 17000/10000: 4.7423944\n",
      "Error loss at 17100/10000: 5.5449557\n",
      "Error loss at 17200/10000: 1.6360297\n",
      "Error loss at 17300/10000: 3.5946379\n",
      "Error loss at 17400/10000: 4.612003\n",
      "Error loss at 17500/10000: 2.6532507\n",
      "Error loss at 17600/10000: 5.4339156\n",
      "Error loss at 17700/10000: 3.9582863\n",
      "Error loss at 17800/10000: 2.3636596\n",
      "Error loss at 17900/10000: 6.4254885\n",
      "Error loss at 18000/10000: 2.910157\n",
      "Error loss at 18100/10000: 6.505145\n",
      "Error loss at 18200/10000: 1.3899934\n",
      "Error loss at 18300/10000: 1.7873523\n",
      "Error loss at 18400/10000: 2.7969594\n",
      "Error loss at 18500/10000: 5.3658414\n",
      "Error loss at 18600/10000: 5.9397144\n",
      "Epoch 2 / 5\n",
      "Error loss at 18700/10000: 4.1084914\n",
      "Error loss at 18800/10000: 1.3862944\n",
      "Error loss at 18900/10000: 2.6999063\n",
      "Error loss at 19000/10000: 4.2704053\n",
      "Error loss at 19100/10000: 3.3151197\n",
      "Error loss at 19200/10000: 1.3862944\n",
      "Error loss at 19300/10000: 5.573948\n",
      "Error loss at 19400/10000: 3.1431909\n",
      "Error loss at 19500/10000: 3.2486084\n",
      "Error loss at 19600/10000: 3.3029094\n",
      "Error loss at 19700/10000: 1.3931367\n",
      "Error loss at 19800/10000: 2.6207666\n",
      "Error loss at 19900/10000: 1.3862948\n",
      "Error loss at 20000/10000: 3.3916383\n",
      "Error loss at 20100/10000: 3.030413\n",
      "Error loss at 20200/10000: 4.844437\n",
      "Error loss at 20300/10000: 1.4725122\n",
      "Error loss at 20400/10000: 1.3862944\n",
      "Error loss at 20500/10000: 2.9189126\n",
      "Error loss at 20600/10000: 6.581894\n",
      "Error loss at 20700/10000: 5.62237\n",
      "Error loss at 20800/10000: 1.3862944\n",
      "Error loss at 20900/10000: 5.268299\n",
      "Error loss at 21000/10000: 5.3660336\n",
      "Error loss at 21100/10000: 2.3277419\n",
      "Error loss at 21200/10000: 6.600173\n",
      "Error loss at 21300/10000: 5.242503\n",
      "Error loss at 21400/10000: 3.9309387\n",
      "Error loss at 21500/10000: 4.8648252\n",
      "Error loss at 21600/10000: 1.6075451\n",
      "Error loss at 21700/10000: 5.9211254\n",
      "Error loss at 21800/10000: 4.776191\n",
      "Error loss at 21900/10000: 5.1003704\n",
      "Error loss at 22000/10000: 2.9411776\n",
      "Error loss at 22100/10000: 4.805573\n",
      "Error loss at 22200/10000: 6.1828775\n",
      "Error loss at 22300/10000: 5.362571\n",
      "Error loss at 22400/10000: 3.033265\n",
      "Error loss at 22500/10000: 3.8073096\n",
      "Error loss at 22600/10000: 2.5841913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loss at 22700/10000: 3.826954\n",
      "Error loss at 22800/10000: 5.6976976\n",
      "Error loss at 22900/10000: 2.8195436\n",
      "Error loss at 23000/10000: 4.4034104\n",
      "Error loss at 23100/10000: 3.5058668\n",
      "Error loss at 23200/10000: 1.3862944\n",
      "Error loss at 23300/10000: 1.3862944\n",
      "Error loss at 23400/10000: 2.2573466\n",
      "Error loss at 23500/10000: 7.8928394\n",
      "Error loss at 23600/10000: 5.2944174\n",
      "Error loss at 23700/10000: 2.441554\n",
      "Error loss at 23800/10000: 1.5060252\n",
      "Error loss at 23900/10000: 3.9567962\n",
      "Error loss at 24000/10000: 1.8140297\n",
      "Error loss at 24100/10000: 4.315318\n",
      "Error loss at 24200/10000: 4.0434556\n",
      "Error loss at 24300/10000: 4.5936894\n",
      "Error loss at 24400/10000: 4.2967005\n",
      "Error loss at 24500/10000: 3.7821474\n",
      "Error loss at 24600/10000: 3.0361805\n",
      "Error loss at 24700/10000: 2.7390106\n",
      "Error loss at 24800/10000: 4.192122\n",
      "Error loss at 24900/10000: 4.563108\n",
      "Error loss at 25000/10000: 5.81782\n",
      "Error loss at 25100/10000: 2.460298\n",
      "Error loss at 25200/10000: 10.017726\n",
      "Error loss at 25300/10000: 4.7716928\n",
      "Error loss at 25400/10000: 5.14911\n",
      "Error loss at 25500/10000: 5.1120787\n",
      "Error loss at 25600/10000: 4.7760572\n",
      "Error loss at 25700/10000: 1.4452434\n",
      "Error loss at 25800/10000: 1.3866111\n",
      "Error loss at 25900/10000: 2.8291326\n",
      "Error loss at 26000/10000: 5.0218325\n",
      "Error loss at 26100/10000: 3.164155\n",
      "Error loss at 26200/10000: 4.5345087\n",
      "Error loss at 26300/10000: 5.645585\n",
      "Error loss at 26400/10000: 4.374142\n",
      "Error loss at 26500/10000: 3.080193\n",
      "Error loss at 26600/10000: 5.4106646\n",
      "Error loss at 26700/10000: 5.4054894\n",
      "Error loss at 26800/10000: 1.6074252\n",
      "Error loss at 26900/10000: 7.2079268\n",
      "Error loss at 27000/10000: 3.7857056\n",
      "Error loss at 27100/10000: 6.3384457\n",
      "Error loss at 27200/10000: 3.8944192\n",
      "Error loss at 27300/10000: 1.4027408\n",
      "Error loss at 27400/10000: 5.78885\n",
      "Error loss at 27500/10000: 1.3863928\n",
      "Error loss at 27600/10000: 2.09919\n",
      "Error loss at 27700/10000: 1.3863215\n",
      "Error loss at 27800/10000: 2.4852679\n",
      "Error loss at 27900/10000: 5.0800657\n",
      "Epoch 3 / 5\n",
      "Error loss at 28000/10000: 5.859874\n",
      "Error loss at 28100/10000: 5.1029797\n",
      "Error loss at 28200/10000: 4.656673\n",
      "Error loss at 28300/10000: 5.021063\n",
      "Error loss at 28400/10000: 1.8932724\n",
      "Error loss at 28500/10000: 5.4526772\n",
      "Error loss at 28600/10000: 4.118353\n",
      "Error loss at 28700/10000: 3.2556424\n",
      "Error loss at 28800/10000: 2.7939336\n",
      "Error loss at 28900/10000: 3.6111875\n",
      "Error loss at 29000/10000: 3.1013458\n",
      "Error loss at 29100/10000: 4.7024846\n",
      "Error loss at 29200/10000: 2.1542275\n",
      "Error loss at 29300/10000: 3.3989468\n",
      "Error loss at 29400/10000: 3.93327\n",
      "Error loss at 29500/10000: 3.990616\n",
      "Error loss at 29600/10000: 5.758955\n",
      "Error loss at 29700/10000: 2.1168249\n",
      "Error loss at 29800/10000: 5.9756756\n",
      "Error loss at 29900/10000: 2.1406112\n",
      "Error loss at 30000/10000: 2.062148\n",
      "Error loss at 30100/10000: 3.1609015\n",
      "Error loss at 30200/10000: 5.182256\n",
      "Error loss at 30300/10000: 5.4925356\n",
      "Error loss at 30400/10000: 1.3893174\n",
      "Error loss at 30500/10000: 2.605735\n",
      "Error loss at 30600/10000: 1.3862944\n",
      "Error loss at 30700/10000: 3.4297616\n",
      "Error loss at 30800/10000: 3.4712374\n",
      "Error loss at 30900/10000: 5.905309\n",
      "Error loss at 31000/10000: 2.860538\n",
      "Error loss at 31100/10000: 4.562089\n",
      "Error loss at 31200/10000: 5.5058813\n",
      "Error loss at 31300/10000: 2.8972077\n",
      "Error loss at 31400/10000: 2.3026981\n",
      "Error loss at 31500/10000: 3.4866438\n",
      "Error loss at 31600/10000: 2.3656738\n",
      "Error loss at 31700/10000: 1.3862944\n",
      "Error loss at 31800/10000: 3.9614182\n",
      "Error loss at 31900/10000: 3.79664\n",
      "Error loss at 32000/10000: 1.4364973\n",
      "Error loss at 32100/10000: 2.9653306\n",
      "Error loss at 32200/10000: 5.846737\n",
      "Error loss at 32300/10000: 3.7800379\n",
      "Error loss at 32400/10000: 4.4420805\n",
      "Error loss at 32500/10000: 2.9716558\n",
      "Error loss at 32600/10000: 8.975322\n",
      "Error loss at 32700/10000: 6.7710943\n",
      "Error loss at 32800/10000: 1.3862944\n",
      "Error loss at 32900/10000: 6.0716286\n",
      "Error loss at 33000/10000: 6.358547\n",
      "Error loss at 33100/10000: 6.8709803\n",
      "Error loss at 33200/10000: 1.3862944\n",
      "Error loss at 33300/10000: 5.103467\n",
      "Error loss at 33400/10000: 1.5531751\n",
      "Error loss at 33500/10000: 4.216941\n",
      "Error loss at 33600/10000: 1.386383\n",
      "Error loss at 33700/10000: 3.9859345\n",
      "Error loss at 33800/10000: 4.072733\n",
      "Error loss at 33900/10000: 2.1152358\n",
      "Error loss at 34000/10000: 4.803741\n",
      "Error loss at 34100/10000: 1.3862944\n",
      "Error loss at 34200/10000: 2.1689909\n",
      "Error loss at 34300/10000: 4.430172\n",
      "Error loss at 34400/10000: 4.810501\n",
      "Error loss at 34500/10000: 2.5043747\n",
      "Error loss at 34600/10000: 5.115443\n",
      "Error loss at 34700/10000: 5.848899\n",
      "Error loss at 34800/10000: 2.7779694\n",
      "Error loss at 34900/10000: 4.735872\n",
      "Error loss at 35000/10000: 7.6335974\n",
      "Error loss at 35100/10000: 3.909566\n",
      "Error loss at 35200/10000: 1.6586393\n",
      "Error loss at 35300/10000: 6.1127005\n",
      "Error loss at 35400/10000: 5.512798\n",
      "Error loss at 35500/10000: 2.7033157\n",
      "Error loss at 35600/10000: 1.3862944\n",
      "Error loss at 35700/10000: 5.2566147\n",
      "Error loss at 35800/10000: 1.9850485\n",
      "Error loss at 35900/10000: 5.231342\n",
      "Error loss at 36000/10000: 3.1604261\n",
      "Error loss at 36100/10000: 2.2147517\n",
      "Error loss at 36200/10000: 3.6373734\n",
      "Error loss at 36300/10000: 1.9003386\n",
      "Error loss at 36400/10000: 4.753598\n",
      "Error loss at 36500/10000: 5.088681\n",
      "Error loss at 36600/10000: 4.416032\n",
      "Error loss at 36700/10000: 5.7468085\n",
      "Error loss at 36800/10000: 5.5443277\n",
      "Error loss at 36900/10000: 3.9968734\n",
      "Error loss at 37000/10000: 3.2490275\n",
      "Error loss at 37100/10000: 3.202458\n",
      "Error loss at 37200/10000: 6.0307336\n",
      "Epoch 4 / 5\n",
      "Error loss at 37300/10000: 4.08857\n",
      "Error loss at 37400/10000: 3.4937873\n",
      "Error loss at 37500/10000: 6.5528135\n",
      "Error loss at 37600/10000: 1.3863279\n",
      "Error loss at 37700/10000: 6.673212\n",
      "Error loss at 37800/10000: 1.3862944\n",
      "Error loss at 37900/10000: 3.8201463\n",
      "Error loss at 38000/10000: 2.4439192\n",
      "Error loss at 38100/10000: 2.7002819\n",
      "Error loss at 38200/10000: 3.0981607\n",
      "Error loss at 38300/10000: 7.5956593\n",
      "Error loss at 38400/10000: 3.6836832\n",
      "Error loss at 38500/10000: 2.7972403\n",
      "Error loss at 38600/10000: 3.7608702\n",
      "Error loss at 38700/10000: 1.5344658\n",
      "Error loss at 38800/10000: 5.5503674\n",
      "Error loss at 38900/10000: 1.758661\n",
      "Error loss at 39000/10000: 1.3862944\n",
      "Error loss at 39100/10000: 3.2028189\n",
      "Error loss at 39200/10000: 3.8495674\n",
      "Error loss at 39300/10000: 3.8861926\n",
      "Error loss at 39400/10000: 3.5176392\n",
      "Error loss at 39500/10000: 2.7238908\n",
      "Error loss at 39600/10000: 6.068805\n",
      "Error loss at 39700/10000: 1.3890522\n",
      "Error loss at 39800/10000: 3.441616\n",
      "Error loss at 39900/10000: 1.7660905\n",
      "Error loss at 40000/10000: 2.7983232\n",
      "Error loss at 40100/10000: 2.5042694\n",
      "Error loss at 40200/10000: 3.4126513\n",
      "Error loss at 40300/10000: 1.3862944\n",
      "Error loss at 40400/10000: 6.397026\n",
      "Error loss at 40500/10000: 1.3862944\n",
      "Error loss at 40600/10000: 2.8122592\n",
      "Error loss at 40700/10000: 3.1064963\n",
      "Error loss at 40800/10000: 1.3862944\n",
      "Error loss at 40900/10000: 1.3884388\n",
      "Error loss at 41000/10000: 1.4760902\n",
      "Error loss at 41100/10000: 2.7418985\n",
      "Error loss at 41200/10000: 4.442049\n",
      "Error loss at 41300/10000: 3.0051763\n",
      "Error loss at 41400/10000: 2.7891362\n",
      "Error loss at 41500/10000: 1.3862944\n",
      "Error loss at 41600/10000: 3.9079735\n",
      "Error loss at 41700/10000: 1.3863163\n",
      "Error loss at 41800/10000: 2.1014543\n",
      "Error loss at 41900/10000: 1.3862944\n",
      "Error loss at 42000/10000: 4.2379127\n",
      "Error loss at 42100/10000: 2.9944334\n",
      "Error loss at 42200/10000: 2.2841725\n",
      "Error loss at 42300/10000: 3.7383318\n",
      "Error loss at 42400/10000: 1.3903869\n",
      "Error loss at 42500/10000: 4.140455\n",
      "Error loss at 42600/10000: 1.900182\n",
      "Error loss at 42700/10000: 3.5530453\n",
      "Error loss at 42800/10000: 4.7968564\n",
      "Error loss at 42900/10000: 1.3862944\n",
      "Error loss at 43000/10000: 1.3862944\n",
      "Error loss at 43100/10000: 4.365433\n",
      "Error loss at 43200/10000: 1.5757569\n",
      "Error loss at 43300/10000: 5.5970707\n",
      "Error loss at 43400/10000: 2.312459\n",
      "Error loss at 43500/10000: 2.3571677\n",
      "Error loss at 43600/10000: 2.5761344\n",
      "Error loss at 43700/10000: 2.2640948\n",
      "Error loss at 43800/10000: 4.34619\n",
      "Error loss at 43900/10000: 1.3862944\n",
      "Error loss at 44000/10000: 4.320326\n",
      "Error loss at 44100/10000: 4.5605183\n",
      "Error loss at 44200/10000: 3.968305\n",
      "Error loss at 44300/10000: 5.962258\n",
      "Error loss at 44400/10000: 2.1150024\n",
      "Error loss at 44500/10000: 2.1026304\n",
      "Error loss at 44600/10000: 1.3872337\n",
      "Error loss at 44700/10000: 1.5184039\n",
      "Error loss at 44800/10000: 3.8946292\n",
      "Error loss at 44900/10000: 3.7499561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loss at 45000/10000: 4.1585712\n",
      "Error loss at 45100/10000: 8.335744\n",
      "Error loss at 45200/10000: 2.0105581\n",
      "Error loss at 45300/10000: 4.0805683\n",
      "Error loss at 45400/10000: 3.220191\n",
      "Error loss at 45500/10000: 4.326351\n",
      "Error loss at 45600/10000: 4.25117\n",
      "Error loss at 45700/10000: 2.6986008\n",
      "Error loss at 45800/10000: 2.1360588\n",
      "Error loss at 45900/10000: 2.583508\n",
      "Error loss at 46000/10000: 2.6649356\n",
      "Error loss at 46100/10000: 4.3716288\n",
      "Error loss at 46200/10000: 2.461509\n",
      "Error loss at 46300/10000: 4.293729\n",
      "Error loss at 46400/10000: 2.689295\n",
      "Error loss at 46500/10000: 7.044636\n",
      "Error loss at 46600/10000: 6.323777\n",
      "Training time: 1068.1752758026123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f88d303acc0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "error_losses = []\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \" + str(epoch) + \" / \" + str(epochs))\n",
    "        np.random.shuffle(sentences)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = [w for w in sentence \\\n",
    "                if np.random.random() < (1 - p_drop[w])\n",
    "            ]\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "            \n",
    "            # randomly order words so we don't always see\n",
    "            # samples in the same order\n",
    "            randomly_ordered_positions = np.random.choice(\n",
    "                len(sentence),\n",
    "                size=len(sentence),\n",
    "                replace=False,\n",
    "            )\n",
    "            \n",
    "            for pos in randomly_ordered_positions:\n",
    "                # the middle word\n",
    "                word = sentence[pos]\n",
    "\n",
    "                # get the positive context words/negative samples\n",
    "                context_words = get_context(pos, sentence, window_size)\n",
    "                neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "                targets = np.array(context_words)                \n",
    "                _, error_loss = sess.run([train, loss], feed_dict={\n",
    "                    pos_input: [word],\n",
    "                    neg_input: [neg_word],\n",
    "                    context_input: context_words\n",
    "                })\n",
    "                \n",
    "            error_losses.append(error_loss)\n",
    "            \n",
    "            if len(error_losses) % 100 == 0:\n",
    "                print(\"Error loss at \" + str(len(error_losses)) + \"/\" + str(len(sentences)) + \": \" + str(error_loss))\n",
    "        \n",
    "        learning_rate -= learning_rate_delta\n",
    "t1 = time.time()\n",
    "print(\"Training time: \" + str(t1-t0))\n",
    "plt.plot(error_losses, color='blue')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(word2idx, W, V):\n",
    "  # there are multiple ways to get the \"final\" word embedding\n",
    "  # We = (W + V.T) / 2\n",
    "  # We = W\n",
    "\n",
    "  idx2word = {i:w for w, i in word2idx.items()}\n",
    "\n",
    "  for We in (W, (W + V.T) / 2):\n",
    "    print(\"**********\")\n",
    "\n",
    "    analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n",
    "    analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n",
    "    analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n",
    "    analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n",
    "    analogy('japan', 'sushi', 'england', 'bread', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n",
    "    analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n",
    "    analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n",
    "    analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n",
    "    analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n",
    "    analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n",
    "    analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n",
    "    analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n",
    "    analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n",
    "    analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n",
    "    analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n",
    "    analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n",
    "    analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n",
    "    analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n",
    "    analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "testing: king - man = queen - woman\n",
      "got: king - man = osman - woman\n",
      "closest 10:\n",
      "king 0.38868016\n",
      "osman 0.44295055\n",
      "prohibits 0.48708928\n",
      "probes 0.5090289\n",
      "classically 0.51096714\n",
      "upon 0.52897453\n",
      "reconstruct 0.530018\n",
      "deck 0.5306651\n",
      "distribution 0.53235215\n",
      "crash 0.5348824\n",
      "dist to queen: 0.9202968776226044\n",
      "testing: king - prince = queen - princess\n",
      "got: king - prince = pas - princess\n",
      "closest 10:\n",
      "princess 0.33188254\n",
      "pas 0.47077936\n",
      "106 0.48913044\n",
      "punctuation 0.497392\n",
      "king 0.50102305\n",
      "intestine 0.5146658\n",
      "pun 0.5190325\n",
      "tentative 0.5227813\n",
      "consecrated 0.5273456\n",
      "singles 0.53417\n",
      "dist to queen: 1.0953312665224075\n",
      "testing: miami - florida = dallas - texas\n",
      "got: miami - florida = 10 - texas\n",
      "closest 10:\n",
      "texas 0.3318866\n",
      "10 0.50429916\n",
      "presses 0.5072386\n",
      "processor 0.5087693\n",
      "layout 0.52151525\n",
      "descartes 0.522684\n",
      "plundered 0.5241772\n",
      "neighbor 0.52692866\n",
      "satyrs 0.5358587\n",
      "massacred 0.53844094\n",
      "dist to dallas: 1.3311095535755157\n",
      "testing: einstein - scientist = picasso - painter\n",
      "got: einstein - scientist = radio - painter\n",
      "closest 10:\n",
      "radio 0.40013236\n",
      "einstein 0.4217643\n",
      "1798 0.47573495\n",
      "painter 0.4920647\n",
      "probe 0.49745297\n",
      "daniels 0.49952286\n",
      "virgil 0.50670457\n",
      "constitutions 0.536536\n",
      "kong 0.5367583\n",
      "satisfies 0.543144\n",
      "dist to picasso: 1.1030545607209206\n",
      "testing: japan - sushi = england - bread\n",
      "got: japan - sushi = vitamins - bread\n",
      "closest 10:\n",
      "japan 0.3416425\n",
      "bread 0.3536898\n",
      "vitamins 0.4983508\n",
      "surrealist 0.52432024\n",
      "enclave 0.52848953\n",
      "illegal 0.5304114\n",
      "honorific 0.5310998\n",
      "ml 0.532352\n",
      "observation 0.5353855\n",
      "legate 0.5377976\n",
      "dist to england: 1.2067147195339203\n",
      "testing: man - woman = he - she\n",
      "got: man - woman = metadata - she\n",
      "closest 10:\n",
      "she 0.28437072\n",
      "metadata 0.44116485\n",
      "drying 0.44189483\n",
      "pictorial 0.461147\n",
      "toxic 0.46239215\n",
      "man 0.47630876\n",
      "ieee 0.4802373\n",
      "commentaries 0.48322296\n",
      "cerebral 0.50845754\n",
      "°c 0.51628256\n",
      "dist to he: 1.009898335672915\n",
      "testing: man - woman = uncle - aunt\n",
      "got: man - woman = syndicalist - aunt\n",
      "closest 10:\n",
      "aunt 0.4506656\n",
      "syndicalist 0.48711008\n",
      "basketball 0.48817748\n",
      "1947 0.49618423\n",
      "nonexistent 0.5105568\n",
      "lantern 0.52240384\n",
      "blockade 0.5224284\n",
      "im 0.5345409\n",
      "coming 0.5354234\n",
      "man 0.5358903\n",
      "dist to uncle: 0.8224302232265472\n",
      "testing: man - woman = brother - sister\n",
      "got: man - woman = slightly - sister\n",
      "closest 10:\n",
      "sister 0.34693825\n",
      "slightly 0.44552368\n",
      "supplemented 0.47234124\n",
      "156 0.53402805\n",
      "shamanism 0.53673303\n",
      "cults 0.5461756\n",
      "equivalent 0.55091584\n",
      "resources 0.5516461\n",
      "support 0.5557464\n",
      "ascension 0.5609602\n",
      "dist to brother: 0.8927199840545654\n",
      "testing: man - woman = husband - wife\n",
      "got: man - woman = malesfemale - wife\n",
      "closest 10:\n",
      "malesfemale 0.35546887\n",
      "wife 0.37537104\n",
      "toxic 0.45048386\n",
      "drying 0.4603505\n",
      "indoeuropean 0.4682641\n",
      "remarked 0.47771394\n",
      "works 0.49147534\n",
      "man 0.49269038\n",
      "honorary 0.5091862\n",
      "equivalent 0.5093548\n",
      "dist to husband: 0.8593807369470596\n",
      "testing: man - woman = actor - actress\n",
      "got: man - woman = metadata - actress\n",
      "closest 10:\n",
      "actress 0.37944317\n",
      "metadata 0.45114893\n",
      "directed 0.48067218\n",
      "adjusted 0.5013207\n",
      "curve 0.525725\n",
      "aryan 0.5283077\n",
      "occupies 0.5334919\n",
      "toxic 0.53528404\n",
      "invading 0.5382365\n",
      "1947 0.53841615\n",
      "dist to actor: 0.9516091383993626\n",
      "testing: man - woman = father - mother\n",
      "got: man - woman = katherine - mother\n",
      "closest 10:\n",
      "mother 0.34358376\n",
      "katherine 0.4550519\n",
      "commentaries 0.47025847\n",
      "man 0.48270476\n",
      "dying 0.5119978\n",
      "recurrent 0.5130925\n",
      "resources 0.51356053\n",
      "conductor 0.5189078\n",
      "wines 0.5206257\n",
      "praying 0.53956914\n",
      "dist to father: 0.8704269230365753\n",
      "testing: heir - heiress = prince - princess\n",
      "Sorry, heiress not in word2idx\n",
      "testing: nephew - niece = uncle - aunt\n",
      "got: nephew - niece = exceeded - aunt\n",
      "closest 10:\n",
      "aunt 0.35684454\n",
      "exceeded 0.49684495\n",
      "befriended 0.4975258\n",
      "europeans 0.50106776\n",
      "greeted 0.5069211\n",
      "months 0.51594734\n",
      "submission 0.52516043\n",
      "purchase 0.53217435\n",
      "halves 0.53505015\n",
      "stakes 0.53803796\n",
      "dist to uncle: 0.7744279652833939\n",
      "testing: france - paris = japan - tokyo\n",
      "got: france - paris = pomerania - tokyo\n",
      "closest 10:\n",
      "tokyo 0.29974973\n",
      "pomerania 0.42116553\n",
      "watching 0.4567436\n",
      "possibility 0.4785192\n",
      "translates 0.48939836\n",
      "tropics 0.49385798\n",
      "quietly 0.49660206\n",
      "can 0.49671382\n",
      "reportedly 0.4996429\n",
      "france 0.50913894\n",
      "dist to japan: 1.2300708889961243\n",
      "testing: france - paris = china - beijing\n",
      "got: france - paris = pop - beijing\n",
      "closest 10:\n",
      "france 0.43289518\n",
      "beijing 0.44758016\n",
      "pop 0.5218696\n",
      "reinforce 0.52479184\n",
      "ignore 0.5284157\n",
      "vegetarian 0.5330167\n",
      "http 0.53450114\n",
      "othello 0.5353499\n",
      "dakota 0.5362767\n",
      "travelling 0.54779184\n",
      "dist to china: 1.1988848447799683\n",
      "testing: february - january = december - november\n",
      "got: february - january = cgi - november\n",
      "closest 10:\n",
      "november 0.3000878\n",
      "february 0.34530097\n",
      "cgi 0.48148996\n",
      "multiplying 0.49752975\n",
      "gdp 0.5027648\n",
      "billions 0.5184033\n",
      "duchess 0.52309597\n",
      "brought 0.5290363\n",
      "sociologists 0.5313274\n",
      "attempt 0.53671515\n",
      "dist to december: 1.3282550275325775\n",
      "testing: france - paris = germany - berlin\n",
      "got: france - paris = yields - berlin\n",
      "closest 10:\n",
      "berlin 0.3264627\n",
      "yields 0.42120862\n",
      "france 0.4433626\n",
      "sing 0.4733615\n",
      "incentives 0.51615965\n",
      "admired 0.52284807\n",
      "motorway 0.52393687\n",
      "dub 0.5412396\n",
      "new 0.54305506\n",
      "registration 0.55239236\n",
      "dist to germany: 1.060722403228283\n",
      "testing: week - day = year - month\n",
      "got: week - day = harmonics - month\n",
      "closest 10:\n",
      "harmonics 0.42900324\n",
      "month 0.445827\n",
      "eastern 0.45086998\n",
      "week 0.456573\n",
      "staged 0.48142648\n",
      "assisted 0.5066104\n",
      "1933 0.52371514\n",
      "myth 0.5250032\n",
      "routes 0.54065686\n",
      "soldiers 0.54151917\n",
      "dist to year: 0.7739763408899307\n",
      "testing: week - day = hour - minute\n",
      "got: week - day = duality - minute\n",
      "closest 10:\n",
      "week 0.29547775\n",
      "minute 0.4671359\n",
      "duality 0.48447013\n",
      "randolph 0.51708615\n",
      "scientists 0.5238781\n",
      "suggesting 0.52699494\n",
      "concealed 0.53178555\n",
      "routes 0.53237295\n",
      "fewest 0.53555\n",
      "naturally 0.5369934\n",
      "dist to hour: 0.8209534883499146\n",
      "testing: france - paris = italy - rome\n",
      "got: france - paris = she - rome\n",
      "closest 10:\n",
      "rome 0.31338644\n",
      "france 0.45298022\n",
      "she 0.5079175\n",
      "registration 0.5133373\n",
      "concessions 0.5188215\n",
      "sheets 0.5188943\n",
      "kilometers 0.52887356\n",
      "homers 0.5408689\n",
      "postseason 0.5455758\n",
      "recurring 0.54600245\n",
      "dist to italy: 0.9302469119429588\n",
      "testing: paris - france = rome - italy\n",
      "got: paris - france = 95 - italy\n",
      "closest 10:\n",
      "paris 0.39127648\n",
      "95 0.49345356\n",
      "atmosphere 0.5133843\n",
      "opinions 0.5210019\n",
      "dress 0.52456945\n",
      "excitement 0.5371323\n",
      "woods 0.5393181\n",
      "positron 0.5536148\n",
      "incorporated 0.5579424\n",
      "afrikaans 0.56006515\n",
      "dist to rome: 1.182822585105896\n",
      "testing: france - french = england - english\n",
      "got: france - french = trinity - english\n",
      "closest 10:\n",
      "france 0.30246\n",
      "english 0.34418786\n",
      "trinity 0.4414925\n",
      "tampa 0.5084402\n",
      "greenhouse 0.5154105\n",
      "replicate 0.531901\n",
      "transitional 0.5374052\n",
      "diverged 0.540095\n",
      "successor 0.5428543\n",
      "antiochus 0.5501909\n",
      "dist to england: 0.9153227657079697\n",
      "testing: japan - japanese = china - chinese\n",
      "got: japan - japanese = circuit - chinese\n",
      "closest 10:\n",
      "chinese 0.3993081\n",
      "circuit 0.4580673\n",
      "japan 0.4596069\n",
      "fortifications 0.48607558\n",
      "crossplatform 0.4940843\n",
      "joke 0.50472045\n",
      "society 0.5127851\n",
      "stark 0.53603184\n",
      "extra 0.5375448\n",
      "acquiring 0.54279363\n",
      "dist to china: 1.0407977476716042\n",
      "testing: china - chinese = america - american\n",
      "got: china - chinese = logo - american\n",
      "closest 10:\n",
      "american 0.34948212\n",
      "logo 0.44848406\n",
      "heinlein 0.4766975\n",
      "exile 0.49193788\n",
      "mahayana 0.5126599\n",
      "kurosawa 0.51916975\n",
      "crest 0.52716553\n",
      "bastarnae 0.5363684\n",
      "rejecting 0.53860176\n",
      "rivals 0.54472244\n",
      "dist to america: 1.1016992777585983\n",
      "testing: japan - japanese = italy - italian\n",
      "got: japan - japanese = salmon - italian\n",
      "closest 10:\n",
      "italian 0.2890516\n",
      "japan 0.44040942\n",
      "salmon 0.4821834\n",
      "eternal 0.48439562\n",
      "fireworks 0.49282157\n",
      "poisonous 0.5165186\n",
      "appropriately 0.52013075\n",
      "burkina 0.52119505\n",
      "pitcairn 0.52811337\n",
      "quechua 0.5284525\n",
      "dist to italy: 0.9628284610807896\n",
      "testing: japan - japanese = australia - australian\n",
      "got: japan - japanese = burgeoning - australian\n",
      "closest 10:\n",
      "japan 0.38171566\n",
      "australian 0.44627297\n",
      "burgeoning 0.515353\n",
      "shows 0.5266621\n",
      "frustration 0.5324899\n",
      "distinctions 0.5354593\n",
      "rookie 0.53938484\n",
      "stew 0.5440955\n",
      "69 0.54594535\n",
      "variations 0.5482366\n",
      "dist to australia: 0.7142007946968079\n",
      "testing: walk - walking = swim - swimming\n",
      "got: walk - walking = heart - swimming\n",
      "closest 10:\n",
      "swimming 0.34082043\n",
      "heart 0.46937454\n",
      "church 0.50208604\n",
      "hierarchy 0.5186664\n",
      "walk 0.52283466\n",
      "headline 0.5351248\n",
      "focusing 0.5393406\n",
      "greene 0.5505628\n",
      "courage 0.55606663\n",
      "ascent 0.5576117\n",
      "dist to swim: 0.8669234812259674\n",
      "**********\n",
      "testing: king - man = queen - woman\n",
      "got: king - man = implication - woman\n",
      "closest 10:\n",
      "king 0.3825258\n",
      "implication 0.5033722\n",
      "mexico 0.51408994\n",
      "wool 0.5143776\n",
      "tactics 0.51864755\n",
      "elf 0.52491397\n",
      "adultery 0.531161\n",
      "volunteered 0.5349803\n",
      "subtypes 0.5349873\n",
      "unfair 0.5430931\n",
      "dist to queen: 1.0057042632251978\n",
      "testing: king - prince = queen - princess\n",
      "got: king - prince = pas - princess\n",
      "closest 10:\n",
      "princess 0.29204452\n",
      "pas 0.40561348\n",
      "heating 0.47885144\n",
      "pong 0.5147724\n",
      "kentucky 0.5212387\n",
      "belgrade 0.542068\n",
      "musée 0.54709506\n",
      "king 0.55577725\n",
      "78 0.5591425\n",
      "genre 0.5606079\n",
      "dist to queen: 1.0019673069473356\n",
      "testing: miami - florida = dallas - texas\n",
      "got: miami - florida = priorities - texas\n",
      "closest 10:\n",
      "texas 0.33069122\n",
      "miami 0.43973905\n",
      "priorities 0.48854595\n",
      "patriarchs 0.52017385\n",
      "constructions 0.53813946\n",
      "sensors 0.5476837\n",
      "memoirs 0.54902446\n",
      "beirut 0.54949844\n",
      "pods 0.55436766\n",
      "indentured 0.557248\n",
      "dist to dallas: 1.0982992500066757\n",
      "testing: einstein - scientist = picasso - painter\n",
      "got: einstein - scientist = protoindoeuropean - painter\n",
      "closest 10:\n",
      "protoindoeuropean 0.4657582\n",
      "singapore 0.47021544\n",
      "decreed 0.506755\n",
      "imaging 0.5182073\n",
      "dissidents 0.5212822\n",
      "emerged 0.5309466\n",
      "linen 0.5414634\n",
      "dijkstra 0.54835474\n",
      "einstein 0.55140007\n",
      "bing 0.5525606\n",
      "dist to picasso: 1.087619885802269\n",
      "testing: japan - sushi = england - bread\n",
      "got: japan - sushi = legate - bread\n",
      "closest 10:\n",
      "japan 0.33381367\n",
      "legate 0.38988572\n",
      "sasanian 0.44674754\n",
      "bread 0.45982122\n",
      "anglicanism 0.5072277\n",
      "evenly 0.51661575\n",
      "owner 0.52277786\n",
      "goddess 0.5278146\n",
      "mainland 0.52908754\n",
      "au 0.53194106\n",
      "dist to england: 1.2190981805324554\n",
      "testing: man - woman = he - she\n",
      "got: man - woman = methodist - she\n",
      "closest 10:\n",
      "she 0.36913985\n",
      "methodist 0.42794997\n",
      "man 0.4673404\n",
      "improving 0.5021559\n",
      "84 0.5070414\n",
      "kangaroos 0.52131796\n",
      "coast 0.52678907\n",
      "gradient 0.5373379\n",
      "pittsburgh 0.5377097\n",
      "dream 0.5412349\n",
      "dist to he: 1.0990453884005547\n",
      "testing: man - woman = uncle - aunt\n",
      "got: man - woman = 120 - aunt\n",
      "closest 10:\n",
      "aunt 0.30989212\n",
      "120 0.5239801\n",
      "man 0.52964073\n",
      "roads 0.5440208\n",
      "opengl 0.5445983\n",
      "latency 0.5447159\n",
      "licensed 0.54834014\n",
      "ignores 0.5497714\n",
      "apostles 0.5508355\n",
      "phage 0.5510903\n",
      "dist to uncle: 0.6623459458351135\n",
      "testing: man - woman = brother - sister\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got: man - woman = transistor - sister\n",
      "closest 10:\n",
      "sister 0.36315298\n",
      "transistor 0.46413928\n",
      "jaguar 0.48483986\n",
      "papal 0.5058025\n",
      "hillary 0.5339289\n",
      "debate 0.53520775\n",
      "15thcentury 0.5487013\n",
      "man 0.5550897\n",
      "fitted 0.55662274\n",
      "microsofts 0.55827916\n",
      "dist to brother: 0.6905505955219269\n",
      "testing: man - woman = husband - wife\n",
      "got: man - woman = architect - wife\n",
      "closest 10:\n",
      "architect 0.5088313\n",
      "ratified 0.5100119\n",
      "methodist 0.5148456\n",
      "wikipedia 0.51876664\n",
      "man 0.5218667\n",
      "societys 0.52311933\n",
      "wife 0.53503656\n",
      "transistor 0.5371397\n",
      "quartet 0.5411476\n",
      "alienation 0.5487287\n",
      "dist to husband: 0.7773336619138718\n",
      "testing: man - woman = actor - actress\n",
      "got: man - woman = corridor - actress\n",
      "closest 10:\n",
      "actress 0.4545914\n",
      "corridor 0.47262174\n",
      "liable 0.5056697\n",
      "dream 0.5106241\n",
      "dots 0.53565645\n",
      "legendary 0.53685045\n",
      "blackjack 0.5388324\n",
      "3rdcentury 0.5439559\n",
      "liquid 0.5516341\n",
      "absorbing 0.555812\n",
      "dist to actor: 0.9354490414261818\n",
      "testing: man - woman = father - mother\n",
      "got: man - woman = rigid - mother\n",
      "closest 10:\n",
      "mother 0.40415478\n",
      "rigid 0.4485951\n",
      "outside 0.5176928\n",
      "monarchs 0.52677333\n",
      "man 0.5321722\n",
      "attaining 0.5432683\n",
      "passage 0.5525263\n",
      "1960s 0.5535923\n",
      "doctorate 0.55576766\n",
      "mixture 0.56754816\n",
      "dist to father: 0.9628544598817825\n",
      "testing: heir - heiress = prince - princess\n",
      "Sorry, heiress not in word2idx\n",
      "testing: nephew - niece = uncle - aunt\n",
      "got: nephew - niece = minute - aunt\n",
      "closest 10:\n",
      "aunt 0.27432936\n",
      "minute 0.39057082\n",
      "political 0.47207427\n",
      "desires 0.4805478\n",
      "euros 0.48736507\n",
      "surfaces 0.51866364\n",
      "panels 0.52013236\n",
      "command 0.52544284\n",
      "eliminating 0.525992\n",
      "eats 0.5282429\n",
      "dist to uncle: 0.7696622759103775\n",
      "testing: france - paris = japan - tokyo\n",
      "got: france - paris = māori - tokyo\n",
      "closest 10:\n",
      "tokyo 0.3108374\n",
      "māori 0.45625204\n",
      "cumulative 0.49321342\n",
      "highest 0.5099231\n",
      "alexios 0.51058173\n",
      "comeback 0.52539426\n",
      "disorder 0.525572\n",
      "platonic 0.5326518\n",
      "sets 0.5327725\n",
      "loyalist 0.5336141\n",
      "dist to japan: 1.0658996105194092\n",
      "testing: france - paris = china - beijing\n",
      "got: france - paris = docking - beijing\n",
      "closest 10:\n",
      "beijing 0.4183058\n",
      "docking 0.44628847\n",
      "loses 0.4869519\n",
      "condition 0.48881477\n",
      "northern 0.49580514\n",
      "france 0.505805\n",
      "hurricane 0.5109859\n",
      "1861 0.52109295\n",
      "decisionmaking 0.53035915\n",
      "epidemic 0.5369966\n",
      "dist to china: 1.1981585919857025\n",
      "testing: february - january = december - november\n",
      "got: february - january = bust - november\n",
      "closest 10:\n",
      "february 0.3420326\n",
      "bust 0.46398807\n",
      "stop 0.47332495\n",
      "rothbard 0.48667926\n",
      "challenge 0.50141627\n",
      "solve 0.50631464\n",
      "retrieval 0.50780684\n",
      "griffin 0.5330391\n",
      "november 0.53969616\n",
      "intends 0.54094666\n",
      "dist to december: 1.2211699783802032\n",
      "testing: france - paris = germany - berlin\n",
      "got: france - paris = comeback - berlin\n",
      "closest 10:\n",
      "berlin 0.46104503\n",
      "comeback 0.4863208\n",
      "registration 0.5037315\n",
      "14thcentury 0.5051599\n",
      "māori 0.50668716\n",
      "france 0.50981534\n",
      "miss 0.5134425\n",
      "iteration 0.5157589\n",
      "certificate 0.5226819\n",
      "diffraction 0.53263795\n",
      "dist to germany: 1.0796226412057877\n",
      "testing: week - day = year - month\n",
      "got: week - day = serial - month\n",
      "closest 10:\n",
      "month 0.41142178\n",
      "serial 0.4942835\n",
      "week 0.50998354\n",
      "icc 0.51320326\n",
      "jains 0.5186473\n",
      "beck 0.5309291\n",
      "1815 0.5359098\n",
      "japan 0.5429331\n",
      "bordeaux 0.5480257\n",
      "russia 0.553568\n",
      "dist to year: 1.0052423672750592\n",
      "testing: week - day = hour - minute\n",
      "got: week - day = impressed - minute\n",
      "closest 10:\n",
      "week 0.33101648\n",
      "impressed 0.5014435\n",
      "1921 0.51036876\n",
      "2200 0.5236719\n",
      "sending 0.5319785\n",
      "injection 0.5387502\n",
      "crochet 0.5431075\n",
      "hair 0.5488326\n",
      "prized 0.54966986\n",
      "possibilities 0.5511396\n",
      "dist to hour: 0.9754870254546404\n",
      "testing: france - paris = italy - rome\n",
      "got: france - paris = traumatic - rome\n",
      "closest 10:\n",
      "rome 0.3609537\n",
      "traumatic 0.4617325\n",
      "disorder 0.50224066\n",
      "intrinsic 0.5063027\n",
      "quite 0.50826967\n",
      "options 0.5094476\n",
      "°c 0.53870755\n",
      "loses 0.53934026\n",
      "highest 0.5467446\n",
      "france 0.5502691\n",
      "dist to italy: 0.9453802965581417\n",
      "testing: paris - france = rome - italy\n",
      "got: paris - france = colonial - italy\n",
      "closest 10:\n",
      "paris 0.3745399\n",
      "colonial 0.46432072\n",
      "italy 0.46757615\n",
      "vatican 0.48472434\n",
      "coat 0.48783648\n",
      "advertising 0.494313\n",
      "sure 0.5078238\n",
      "transliteration 0.5233635\n",
      "sleeves 0.525632\n",
      "foods 0.5381252\n",
      "dist to rome: 1.111029252409935\n",
      "testing: france - french = england - english\n",
      "got: france - french = wing - english\n",
      "closest 10:\n",
      "english 0.33255875\n",
      "france 0.36404496\n",
      "wing 0.48558956\n",
      "indicative 0.50887394\n",
      "instructed 0.52901435\n",
      "undoubtedly 0.53526187\n",
      "utterly 0.5367322\n",
      "nonempty 0.5431679\n",
      "revolves 0.54992044\n",
      "pentium 0.55016166\n",
      "dist to england: 1.10193482786417\n",
      "testing: japan - japanese = china - chinese\n",
      "got: japan - japanese = suspected - chinese\n",
      "closest 10:\n",
      "chinese 0.24040103\n",
      "suspected 0.47876716\n",
      "133 0.5016134\n",
      "hijackers 0.5022424\n",
      "japan 0.5122894\n",
      "1841 0.5182811\n",
      "australia 0.5298897\n",
      "arsenic 0.5503438\n",
      "fianna 0.5517306\n",
      "claudius 0.5533321\n",
      "dist to china: 1.1406683176755905\n",
      "testing: china - chinese = america - american\n",
      "got: china - chinese = uzbekistan - american\n",
      "closest 10:\n",
      "american 0.44562405\n",
      "uzbekistan 0.4470471\n",
      "china 0.46496522\n",
      "humanists 0.5100541\n",
      "dressing 0.51539457\n",
      "nonviolent 0.5170534\n",
      "q 0.52461636\n",
      "lecture 0.5276085\n",
      "necessitated 0.5336346\n",
      "advancement 0.5379434\n",
      "dist to america: 1.0037379588466138\n",
      "testing: japan - japanese = italy - italian\n",
      "got: japan - japanese = phonograph - italian\n",
      "closest 10:\n",
      "italian 0.31409848\n",
      "phonograph 0.51326513\n",
      "personally 0.5267254\n",
      "monet 0.52713263\n",
      "facilities 0.5281366\n",
      "compartment 0.53262\n",
      "prime 0.53321266\n",
      "vibrations 0.53387314\n",
      "prefixes 0.5438708\n",
      "tongue 0.5509166\n",
      "dist to italy: 0.99065478798002\n",
      "testing: japan - japanese = australia - australian\n",
      "got: japan - japanese = receive - australian\n",
      "closest 10:\n",
      "australian 0.38586533\n",
      "japan 0.42463642\n",
      "receive 0.47040343\n",
      "should 0.49544293\n",
      "australia 0.49829513\n",
      "ballet 0.5125\n",
      "armys 0.527629\n",
      "schwartz 0.52968\n",
      "prophetic 0.54696345\n",
      "phonograph 0.54957193\n",
      "dist to australia: 0.4982950687408447\n",
      "testing: walk - walking = swim - swimming\n",
      "got: walk - walking = gibraltar - swimming\n",
      "closest 10:\n",
      "gibraltar 0.44881356\n",
      "walk 0.45131922\n",
      "cube 0.45333028\n",
      "glen 0.49346262\n",
      "dominican 0.5114192\n",
      "restriction 0.5202488\n",
      "163 0.5220674\n",
      "swimming 0.5250583\n",
      "comedic 0.52648145\n",
      "expectation 0.5274725\n",
      "dist to swim: 0.987936845049262\n"
     ]
    }
   ],
   "source": [
    "test_model(word2idx, W, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
