{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neural Networks to Solve NLP Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-speech tagging (POS)\n",
    "\n",
    "- Assign a category to a word according to its syntactic function.\n",
    "    - noun, pronoun, adjective, determiner, verb, adverb, preposition, conjunction, interjection\n",
    "- Data download link: https://www.clips.uantwerpen.be/conll2000/chunking/\n",
    "- F1 score: \n",
    "\n",
    "$$ F1 = 2 \\frac{precision * recall}{precision + recall} $$\n",
    "$$ precision = \\frac{TruePositives}{TruePositives + FalsePositives} $$\n",
    "$$ recall = \\frac{TruePositives}{TruePositives + FalseNegatives} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    dataset = dataset.drop('drop', axis=1)\n",
    "    dataset['word'] = dataset['word'].apply(lambda x: x.lower())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = preprocess(pd.read_csv('../large_files/chunking/train.txt', sep=' ', names=['word', 'tag', 'drop']))\n",
    "testData = preprocess(pd.read_csv('../large_files/chunking/test.txt', sep=' ', names=['word', 'tag', 'drop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confidence</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pound</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  tag\n",
       "0  confidence   NN\n",
       "1          in   IN\n",
       "2         the   DT\n",
       "3       pound   NN\n",
       "4          is  VBZ"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = trainData['word'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = trainData['tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NN', 'IN', 'DT', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'JJ', 'NNS',\n",
       "       'NNP', ',', 'CC', 'POS', '.', 'VBP', 'VBG', 'PRP$', 'CD', '``',\n",
       "       \"''\", 'VBD', 'EX', 'MD', '#', '(', '$', ')', 'NNPS', 'PRP', 'JJS',\n",
       "       'WP', 'RBR', 'JJR', 'WDT', 'WRB', 'RBS', 'PDT', 'RP', ':', 'FW',\n",
       "       'WP$', 'SYM', 'UH'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "- Does not capture sequence information: \n",
    "    - p(tag | word) = softmax(W[word_index])\n",
    "- It just maps one single word to one tag. \n",
    "- Ambiguities are not treated by this model\n",
    "    - A word having more than one possible tag\n",
    "    - \"Book a ship to france\"\n",
    "    - \"Ship a book to france\"\n",
    "- Accuracy: > 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, vocab_list, tag_list, epochs=10, batch_size=100):\n",
    "        features = [\n",
    "            tf.feature_column.categorical_column_with_vocabulary_list('word', vocabulary_list=vocab_list)\n",
    "        ]\n",
    "        self.model = tf.estimator.LinearClassifier(feature_columns=features, n_classes=len(tag_list), label_vocabulary=tag_list)\n",
    "        input_func = tf.estimator.inputs.pandas_input_fn(x=X,y=Y,batch_size=batch_size,num_epochs=epochs,shuffle=True)\n",
    "        self.model.train(input_func, steps=epochs*len(X)/batch_size)\n",
    "\n",
    "    def evaluate(self, X, Y, batch_size=10):\n",
    "        eval_input_func = tf.estimator.inputs.pandas_input_fn(\n",
    "            x=X,\n",
    "            y=Y,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        results = self.model.evaluate(eval_input_func)\n",
    "        return results\n",
    "\n",
    "    def predict(self, words):\n",
    "        pred_input_func = tf.estimator.inputs.pandas_input_fn(\n",
    "              x=pd.DataFrame.from_dict({'word': words}),\n",
    "              batch_size=100,\n",
    "              num_epochs=1,\n",
    "              shuffle=False\n",
    "        )\n",
    "        predictions = self.model.predict(pred_input_func)\n",
    "        return list(predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmphd4s7hb3\n",
      "INFO:tensorflow:Using config: {'_global_id_in_cluster': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_eval_distribute': None, '_num_worker_replicas': 1, '_model_dir': '/tmp/tmphd4s7hb3', '_is_chief': True, '_save_summary_steps': 100, '_tf_random_seed': None, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_service': None, '_evaluation_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_experimental_distribute': None, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7a80638f28>, '_protocol': None, '_device_fn': None, '_log_step_count_steps': 100, '_task_id': 0, '_master': '', '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmphd4s7hb3/model.ckpt.\n",
      "INFO:tensorflow:loss = 378.41898, step = 1\n",
      "INFO:tensorflow:global_step/sec: 250.013\n",
      "INFO:tensorflow:loss = 175.13567, step = 101 (0.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.721\n",
      "INFO:tensorflow:loss = 115.27223, step = 201 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.5\n",
      "INFO:tensorflow:loss = 99.46385, step = 301 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.615\n",
      "INFO:tensorflow:loss = 111.201294, step = 401 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.235\n",
      "INFO:tensorflow:loss = 104.31864, step = 501 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.438\n",
      "INFO:tensorflow:loss = 99.747444, step = 601 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.754\n",
      "INFO:tensorflow:loss = 81.38308, step = 701 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.339\n",
      "INFO:tensorflow:loss = 113.89626, step = 801 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.624\n",
      "INFO:tensorflow:loss = 81.329124, step = 901 (0.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.913\n",
      "INFO:tensorflow:loss = 87.07567, step = 1001 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.638\n",
      "INFO:tensorflow:loss = 93.336945, step = 1101 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.992\n",
      "INFO:tensorflow:loss = 89.6155, step = 1201 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.125\n",
      "INFO:tensorflow:loss = 92.077484, step = 1301 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.697\n",
      "INFO:tensorflow:loss = 82.33485, step = 1401 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.039\n",
      "INFO:tensorflow:loss = 48.949768, step = 1501 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.496\n",
      "INFO:tensorflow:loss = 70.6137, step = 1601 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.957\n",
      "INFO:tensorflow:loss = 76.18465, step = 1701 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.925\n",
      "INFO:tensorflow:loss = 68.68292, step = 1801 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.448\n",
      "INFO:tensorflow:loss = 84.56786, step = 1901 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.357\n",
      "INFO:tensorflow:loss = 60.273045, step = 2001 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.22\n",
      "INFO:tensorflow:loss = 77.70564, step = 2101 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.496\n",
      "INFO:tensorflow:loss = 64.4099, step = 2201 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.344\n",
      "INFO:tensorflow:loss = 50.43498, step = 2301 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.279\n",
      "INFO:tensorflow:loss = 57.232063, step = 2401 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.183\n",
      "INFO:tensorflow:loss = 61.233166, step = 2501 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.369\n",
      "INFO:tensorflow:loss = 54.77946, step = 2601 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.965\n",
      "INFO:tensorflow:loss = 48.578438, step = 2701 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.195\n",
      "INFO:tensorflow:loss = 67.343285, step = 2801 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.046\n",
      "INFO:tensorflow:loss = 61.00597, step = 2901 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.521\n",
      "INFO:tensorflow:loss = 64.17991, step = 3001 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.992\n",
      "INFO:tensorflow:loss = 52.280857, step = 3101 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.037\n",
      "INFO:tensorflow:loss = 43.110954, step = 3201 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.558\n",
      "INFO:tensorflow:loss = 55.046246, step = 3301 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.239\n",
      "INFO:tensorflow:loss = 61.483315, step = 3401 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.251\n",
      "INFO:tensorflow:loss = 61.310528, step = 3501 (0.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.195\n",
      "INFO:tensorflow:loss = 40.758995, step = 3601 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.396\n",
      "INFO:tensorflow:loss = 64.12301, step = 3701 (0.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.886\n",
      "INFO:tensorflow:loss = 47.63024, step = 3801 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.746\n",
      "INFO:tensorflow:loss = 35.893044, step = 3901 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.534\n",
      "INFO:tensorflow:loss = 54.75361, step = 4001 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.616\n",
      "INFO:tensorflow:loss = 57.38589, step = 4101 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.875\n",
      "INFO:tensorflow:loss = 55.771248, step = 4201 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.502\n",
      "INFO:tensorflow:loss = 54.943092, step = 4301 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.814\n",
      "INFO:tensorflow:loss = 51.890427, step = 4401 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.639\n",
      "INFO:tensorflow:loss = 50.504295, step = 4501 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.489\n",
      "INFO:tensorflow:loss = 44.61165, step = 4601 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.06\n",
      "INFO:tensorflow:loss = 42.36319, step = 4701 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.66\n",
      "INFO:tensorflow:loss = 60.693092, step = 4801 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.611\n",
      "INFO:tensorflow:loss = 54.55261, step = 4901 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.545\n",
      "INFO:tensorflow:loss = 57.616577, step = 5001 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.723\n",
      "INFO:tensorflow:loss = 33.96763, step = 5101 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.256\n",
      "INFO:tensorflow:loss = 35.870537, step = 5201 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.824\n",
      "INFO:tensorflow:loss = 52.513092, step = 5301 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.795\n",
      "INFO:tensorflow:loss = 36.150963, step = 5401 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.427\n",
      "INFO:tensorflow:loss = 52.639446, step = 5501 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.938\n",
      "INFO:tensorflow:loss = 47.957737, step = 5601 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.226\n",
      "INFO:tensorflow:loss = 46.14966, step = 5701 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.337\n",
      "INFO:tensorflow:loss = 42.29928, step = 5801 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.433\n",
      "INFO:tensorflow:loss = 43.545685, step = 5901 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.157\n",
      "INFO:tensorflow:loss = 41.948204, step = 6001 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.602\n",
      "INFO:tensorflow:loss = 53.803635, step = 6101 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.011\n",
      "INFO:tensorflow:loss = 32.13486, step = 6201 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.01\n",
      "INFO:tensorflow:loss = 42.484398, step = 6301 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.453\n",
      "INFO:tensorflow:loss = 46.75588, step = 6401 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.293\n",
      "INFO:tensorflow:loss = 40.158234, step = 6501 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.072\n",
      "INFO:tensorflow:loss = 32.116158, step = 6601 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.902\n",
      "INFO:tensorflow:loss = 40.18346, step = 6701 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.734\n",
      "INFO:tensorflow:loss = 25.949627, step = 6801 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.546\n",
      "INFO:tensorflow:loss = 48.37149, step = 6901 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.741\n",
      "INFO:tensorflow:loss = 43.43686, step = 7001 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 30.987915, step = 7101 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.047\n",
      "INFO:tensorflow:loss = 39.588947, step = 7201 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.925\n",
      "INFO:tensorflow:loss = 29.076149, step = 7301 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.277\n",
      "INFO:tensorflow:loss = 39.514095, step = 7401 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.123\n",
      "INFO:tensorflow:loss = 35.54767, step = 7501 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.236\n",
      "INFO:tensorflow:loss = 35.500275, step = 7601 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.017\n",
      "INFO:tensorflow:loss = 57.660866, step = 7701 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.491\n",
      "INFO:tensorflow:loss = 42.751644, step = 7801 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.021\n",
      "INFO:tensorflow:loss = 35.18807, step = 7901 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.665\n",
      "INFO:tensorflow:loss = 55.695854, step = 8001 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.105\n",
      "INFO:tensorflow:loss = 39.37441, step = 8101 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.047\n",
      "INFO:tensorflow:loss = 30.056963, step = 8201 (0.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.705\n",
      "INFO:tensorflow:loss = 49.103195, step = 8301 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.389\n",
      "INFO:tensorflow:loss = 26.209663, step = 8401 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.586\n",
      "INFO:tensorflow:loss = 41.271908, step = 8501 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.867\n",
      "INFO:tensorflow:loss = 36.313187, step = 8601 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.758\n",
      "INFO:tensorflow:loss = 42.05391, step = 8701 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.788\n",
      "INFO:tensorflow:loss = 39.436657, step = 8801 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.21\n",
      "INFO:tensorflow:loss = 29.50989, step = 8901 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.443\n",
      "INFO:tensorflow:loss = 36.761482, step = 9001 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.786\n",
      "INFO:tensorflow:loss = 36.03271, step = 9101 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.335\n",
      "INFO:tensorflow:loss = 54.254463, step = 9201 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.986\n",
      "INFO:tensorflow:loss = 21.570314, step = 9301 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.405\n",
      "INFO:tensorflow:loss = 39.91288, step = 9401 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.979\n",
      "INFO:tensorflow:loss = 32.700794, step = 9501 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.061\n",
      "INFO:tensorflow:loss = 40.281803, step = 9601 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.816\n",
      "INFO:tensorflow:loss = 54.89639, step = 9701 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.879\n",
      "INFO:tensorflow:loss = 55.875233, step = 9801 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.257\n",
      "INFO:tensorflow:loss = 33.888832, step = 9901 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.395\n",
      "INFO:tensorflow:loss = 29.784924, step = 10001 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.811\n",
      "INFO:tensorflow:loss = 35.032383, step = 10101 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.759\n",
      "INFO:tensorflow:loss = 32.50654, step = 10201 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.662\n",
      "INFO:tensorflow:loss = 34.39039, step = 10301 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.134\n",
      "INFO:tensorflow:loss = 47.093945, step = 10401 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.821\n",
      "INFO:tensorflow:loss = 35.79068, step = 10501 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 271.268\n",
      "INFO:tensorflow:loss = 37.394337, step = 10601 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.139\n",
      "INFO:tensorflow:loss = 43.52201, step = 10701 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.28\n",
      "INFO:tensorflow:loss = 34.717022, step = 10801 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.706\n",
      "INFO:tensorflow:loss = 22.969286, step = 10901 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.901\n",
      "INFO:tensorflow:loss = 49.93352, step = 11001 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.316\n",
      "INFO:tensorflow:loss = 25.176018, step = 11101 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.7\n",
      "INFO:tensorflow:loss = 32.41032, step = 11201 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.927\n",
      "INFO:tensorflow:loss = 29.820276, step = 11301 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.381\n",
      "INFO:tensorflow:loss = 33.196327, step = 11401 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.083\n",
      "INFO:tensorflow:loss = 26.328302, step = 11501 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.215\n",
      "INFO:tensorflow:loss = 24.863323, step = 11601 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.446\n",
      "INFO:tensorflow:loss = 20.726225, step = 11701 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.955\n",
      "INFO:tensorflow:loss = 36.95693, step = 11801 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.9\n",
      "INFO:tensorflow:loss = 22.441452, step = 11901 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.025\n",
      "INFO:tensorflow:loss = 31.257229, step = 12001 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.433\n",
      "INFO:tensorflow:loss = 36.064705, step = 12101 (0.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.597\n",
      "INFO:tensorflow:loss = 43.468037, step = 12201 (0.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.895\n",
      "INFO:tensorflow:loss = 38.74566, step = 12301 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.213\n",
      "INFO:tensorflow:loss = 23.57779, step = 12401 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.813\n",
      "INFO:tensorflow:loss = 26.535519, step = 12501 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.553\n",
      "INFO:tensorflow:loss = 19.052979, step = 12601 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.85\n",
      "INFO:tensorflow:loss = 35.673977, step = 12701 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.553\n",
      "INFO:tensorflow:loss = 40.602196, step = 12801 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.531\n",
      "INFO:tensorflow:loss = 30.263968, step = 12901 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.37\n",
      "INFO:tensorflow:loss = 19.828857, step = 13001 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.043\n",
      "INFO:tensorflow:loss = 32.87648, step = 13101 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.005\n",
      "INFO:tensorflow:loss = 35.186943, step = 13201 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.577\n",
      "INFO:tensorflow:loss = 37.296234, step = 13301 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.121\n",
      "INFO:tensorflow:loss = 21.359951, step = 13401 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.519\n",
      "INFO:tensorflow:loss = 37.18804, step = 13501 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.863\n",
      "INFO:tensorflow:loss = 28.80053, step = 13601 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.372\n",
      "INFO:tensorflow:loss = 33.483772, step = 13701 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.73\n",
      "INFO:tensorflow:loss = 27.868843, step = 13801 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 271.004\n",
      "INFO:tensorflow:loss = 29.696419, step = 13901 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.656\n",
      "INFO:tensorflow:loss = 47.19111, step = 14001 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.697\n",
      "INFO:tensorflow:loss = 32.75235, step = 14101 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.613\n",
      "INFO:tensorflow:loss = 33.05625, step = 14201 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.748\n",
      "INFO:tensorflow:loss = 38.569935, step = 14301 (0.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.65\n",
      "INFO:tensorflow:loss = 30.32988, step = 14401 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.385\n",
      "INFO:tensorflow:loss = 21.30791, step = 14501 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.752\n",
      "INFO:tensorflow:loss = 39.61463, step = 14601 (0.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.414\n",
      "INFO:tensorflow:loss = 29.803076, step = 14701 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.222\n",
      "INFO:tensorflow:loss = 31.116379, step = 14801 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.111\n",
      "INFO:tensorflow:loss = 28.930378, step = 14901 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.471\n",
      "INFO:tensorflow:loss = 23.445017, step = 15001 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.732\n",
      "INFO:tensorflow:loss = 28.400427, step = 15101 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.633\n",
      "INFO:tensorflow:loss = 30.515932, step = 15201 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.231\n",
      "INFO:tensorflow:loss = 34.255165, step = 15301 (0.410 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 226.124\n",
      "INFO:tensorflow:loss = 20.012203, step = 15401 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.853\n",
      "INFO:tensorflow:loss = 35.40974, step = 15501 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.678\n",
      "INFO:tensorflow:loss = 41.794556, step = 15601 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.052\n",
      "INFO:tensorflow:loss = 37.339996, step = 15701 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.925\n",
      "INFO:tensorflow:loss = 18.189425, step = 15801 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.724\n",
      "INFO:tensorflow:loss = 33.17208, step = 15901 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.913\n",
      "INFO:tensorflow:loss = 35.617386, step = 16001 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.552\n",
      "INFO:tensorflow:loss = 33.488907, step = 16101 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.908\n",
      "INFO:tensorflow:loss = 32.82385, step = 16201 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.49\n",
      "INFO:tensorflow:loss = 21.339346, step = 16301 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.06\n",
      "INFO:tensorflow:loss = 17.284256, step = 16401 (0.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.399\n",
      "INFO:tensorflow:loss = 29.92346, step = 16501 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.12\n",
      "INFO:tensorflow:loss = 20.152496, step = 16601 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.876\n",
      "INFO:tensorflow:loss = 27.133755, step = 16701 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.698\n",
      "INFO:tensorflow:loss = 27.363974, step = 16801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.392\n",
      "INFO:tensorflow:loss = 25.82988, step = 16901 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.979\n",
      "INFO:tensorflow:loss = 23.4597, step = 17001 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.834\n",
      "INFO:tensorflow:loss = 33.395622, step = 17101 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.457\n",
      "INFO:tensorflow:loss = 39.604954, step = 17201 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.324\n",
      "INFO:tensorflow:loss = 21.274809, step = 17301 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.159\n",
      "INFO:tensorflow:loss = 30.895432, step = 17401 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.377\n",
      "INFO:tensorflow:loss = 38.784866, step = 17501 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.512\n",
      "INFO:tensorflow:loss = 39.076065, step = 17601 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.571\n",
      "INFO:tensorflow:loss = 34.565273, step = 17701 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.577\n",
      "INFO:tensorflow:loss = 28.853271, step = 17801 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.304\n",
      "INFO:tensorflow:loss = 22.532152, step = 17901 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.552\n",
      "INFO:tensorflow:loss = 22.054935, step = 18001 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.713\n",
      "INFO:tensorflow:loss = 28.515963, step = 18101 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.721\n",
      "INFO:tensorflow:loss = 29.282238, step = 18201 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.102\n",
      "INFO:tensorflow:loss = 36.65966, step = 18301 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.269\n",
      "INFO:tensorflow:loss = 19.929937, step = 18401 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.549\n",
      "INFO:tensorflow:loss = 34.04266, step = 18501 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.168\n",
      "INFO:tensorflow:loss = 22.424131, step = 18601 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.353\n",
      "INFO:tensorflow:loss = 38.68982, step = 18701 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.351\n",
      "INFO:tensorflow:loss = 28.333643, step = 18801 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.904\n",
      "INFO:tensorflow:loss = 22.076021, step = 18901 (0.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.893\n",
      "INFO:tensorflow:loss = 23.78278, step = 19001 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.386\n",
      "INFO:tensorflow:loss = 25.405436, step = 19101 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.165\n",
      "INFO:tensorflow:loss = 26.460258, step = 19201 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.79\n",
      "INFO:tensorflow:loss = 25.085526, step = 19301 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.699\n",
      "INFO:tensorflow:loss = 39.14089, step = 19401 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.29\n",
      "INFO:tensorflow:loss = 19.676702, step = 19501 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.145\n",
      "INFO:tensorflow:loss = 30.16071, step = 19601 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.029\n",
      "INFO:tensorflow:loss = 43.603813, step = 19701 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.317\n",
      "INFO:tensorflow:loss = 20.382032, step = 19801 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.08\n",
      "INFO:tensorflow:loss = 21.102104, step = 19901 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.723\n",
      "INFO:tensorflow:loss = 23.686668, step = 20001 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.688\n",
      "INFO:tensorflow:loss = 28.84106, step = 20101 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.551\n",
      "INFO:tensorflow:loss = 37.752754, step = 20201 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.331\n",
      "INFO:tensorflow:loss = 24.959265, step = 20301 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.654\n",
      "INFO:tensorflow:loss = 23.720312, step = 20401 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.798\n",
      "INFO:tensorflow:loss = 46.040386, step = 20501 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.444\n",
      "INFO:tensorflow:loss = 35.67221, step = 20601 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.277\n",
      "INFO:tensorflow:loss = 39.398537, step = 20701 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.73\n",
      "INFO:tensorflow:loss = 29.493748, step = 20801 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.809\n",
      "INFO:tensorflow:loss = 24.7968, step = 20901 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.731\n",
      "INFO:tensorflow:loss = 31.48389, step = 21001 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.527\n",
      "INFO:tensorflow:loss = 28.789637, step = 21101 (0.387 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21173 into /tmp/tmphd4s7hb3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 14.650518.\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X=trainData, Y=trainData['tag'], vocab_list=vocabulary.tolist(), tag_list=tags.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-02-16:02:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmphd4s7hb3/model.ckpt-21173\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-02-16:02:31\n",
      "INFO:tensorflow:Saving dict for global step 21173: accuracy = 0.89351374, average_loss = 0.3801966, global_step = 21173, loss = 3.8017251\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 21173: /tmp/tmphd4s7hb3/model.ckpt-21173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.89351374,\n",
       " 'average_loss': 0.3801966,\n",
       " 'global_step': 21173,\n",
       " 'loss': 3.8017251}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X=testData, Y=testData['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmphd4s7hb3/model.ckpt-21173\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'book': 'NN',\n",
       " 'car': 'NN',\n",
       " 'house': 'NNP',\n",
       " 'of': 'IN',\n",
       " 'really': 'RB',\n",
       " 'run': 'VB',\n",
       " 'ship': 'NN'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['car', 'book', 'house', 'run', 'ship', 'of', 'really']\n",
    "predictions = model.predict(words)\n",
    "dict(zip(\n",
    "    words, \n",
    "    [ p['classes'][0].decode('utf-8') for p in predictions ]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks\n",
    "\n",
    "- Use sequences, use context\n",
    "- Similar to logistic regression model, but with an output entering again to the RNN\n",
    "\n",
    "$$ h(t) = \\sigma(W_x^T x(t) + W_h^T h(t-1) + b) $$\n",
    "\n",
    "- Modern RNNs:\n",
    "    - LSTMs\n",
    "    - GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path='../large_files', split_sequences=False):\n",
    "    if not os.path.exists(path + '/chunking'):\n",
    "        print(\"Please create a folder in your local directory called 'chunking'\")\n",
    "        print(\"train.txt and test.txt should be stored in there.\")\n",
    "        print(\"Please check the comments to get the download link.\")\n",
    "        exit()\n",
    "    elif not os.path.exists(path + '/chunking/train.txt'):\n",
    "        print(\"train.txt is not in chunking/train.txt\")\n",
    "        print(\"Please check the comments to get the download link.\")\n",
    "        exit()\n",
    "    elif not os.path.exists(path + '/chunking/test.txt'):\n",
    "        print(\"test.txt is not in chunking/test.txt\")\n",
    "        print(\"Please check the comments to get the download link.\")\n",
    "        exit()\n",
    "\n",
    "    word2idx = {}\n",
    "    tag2idx = {}\n",
    "    word_idx = 1\n",
    "    tag_idx = 1\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line in open(path + '/chunking/train.txt'):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.lower().split()\n",
    "            word, tag, _ = r\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = word_idx\n",
    "                word_idx += 1\n",
    "            currentX.append(word2idx[word])\n",
    "            \n",
    "            if tag not in tag2idx:\n",
    "                tag2idx[tag] = tag_idx\n",
    "                tag_idx += 1\n",
    "            currentY.append(tag2idx[tag])\n",
    "        elif split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "\n",
    "    if not split_sequences:\n",
    "        Xtrain = currentX\n",
    "        Ytrain = currentY\n",
    "\n",
    "    # load and score test data\n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    unknownIdx = word_idx\n",
    "    for line in open(path + '/chunking/test.txt'):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.lower().split()\n",
    "            word, tag, _ = r\n",
    "            if word in word2idx:\n",
    "                currentX.append(word2idx[word])\n",
    "            else:\n",
    "                currentX.append(unknownIdx) # use this as unknown\n",
    "            currentY.append(tag2idx[tag])\n",
    "        elif split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "    if not split_sequences:\n",
    "        Xtest = currentX\n",
    "        Ytest = currentY\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx, unknownIdx\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx, unknownIdx = get_data(split_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(word2idx) + 2\n",
    "K = len(set(flatten(Ytrain)) | set(flatten(Ytest))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, sentences, tags, vocab_size, num_tags, hidden_layer_size=10, embedding_dim=10, batch_size=32, epochs=10, lr=1e-2):\n",
    "        self.sequence_length = max(len(x) for x in sentences)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape=[None, self.sequence_length])\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, self.sequence_length])\n",
    "        \n",
    "        We = tf.Variable(np.random.randn(vocab_size, embedding_dim).astype(np.float32))\n",
    "        Wo = tf.Variable(np.random.randn(hidden_layer_size, num_tags).astype(np.float32))\n",
    "        bo = tf.Variable(np.random.randn(num_tags).astype(np.float32))\n",
    "        x = tf.nn.embedding_lookup(We, self.inputs) # batch_size x sequence_length x embedding_dim\n",
    "        x = tf.unstack(x, self.sequence_length, 1)  # sequence_length x batch_size x embedding_dim\n",
    "        \n",
    "        rnn_unit = tf.contrib.rnn.LSTMCell(hidden_layer_size, activation=tf.nn.relu, dtype=tf.float32)\n",
    "        outputs, _ = tf.contrib.rnn.static_rnn(rnn_unit, x, dtype=tf.float32) # sequence_length x batch_size x hidden_layer_size\n",
    "        outputs = tf.transpose(outputs, (1, 0, 2)) # batch_size x sequence_length x hidden_layer_size\n",
    "        outputs = tf.reshape(outputs, (self.sequence_length * batch_size, hidden_layer_size)) # NT x hidden_layer_size\n",
    "        \n",
    "        logits = tf.matmul(outputs, Wo) + bo # NT x K\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        self.predict_op = tf.reshape(predictions, (batch_size, self.sequence_length))\n",
    "        labels_flat = tf.reshape(self.targets, [-1])\n",
    "        \n",
    "        cost_op = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=tf.reduce_max(logits, 1),\n",
    "                labels=labels_flat, \n",
    "            )\n",
    "        )\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        train_op = optimizer.minimize(cost_op)\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        t0 = time.time()\n",
    "        padSents = tf.keras.preprocessing.sequence.pad_sequences(sentences, maxlen=self.sequence_length)\n",
    "        padTags = tf.keras.preprocessing.sequence.pad_sequences(tags, maxlen=self.sequence_length)\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "        costs = []\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Starting epoch {} of {}\".format(epoch, epochs))\n",
    "            # padSents, padTags = shuffle(padSents, padTags)\n",
    "            for b in range(len(padSents) // batch_size):\n",
    "                start = b*batch_size\n",
    "                end = (b+1)*batch_size\n",
    "                _, cost = self.sess.run([train_op, cost_op], feed_dict={\n",
    "                    self.inputs: padSents[start:end],\n",
    "                    self.targets: padTags[start:end]\n",
    "                })\n",
    "                costs.append(cost)\n",
    "        t1 = time.time()\n",
    "        print(\"Training time: \" + str(t1 - t0))\n",
    "        plt.plot(costs)\n",
    "        \n",
    "    def predict(self, sentences):\n",
    "        padSentences = tf.keras.preprocessing.sequence.pad_sequences(sentences, maxlen=self.sequence_length)\n",
    "        return self.sess.run(self.predict_op, feed_dict={inputs: padSentences})     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-56ad4feddb58>:31: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Starting epoch 0 of 2\n",
      "Starting epoch 1 of 2\n",
      "Training time: 13.128221988677979\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAFG1JREFUeJzt3W2MXOd5n/Hr3l2SS1KUSFkbmdLSpREkMlTDlpyN8+LCaOy6kGPXzge3ldEEcRuAKJC6dpEgsNEPRQukSNEicNAXB4Tj2GhkGYlioYHhuBJip65Rx+1KVhRZct4cxWcoSlxlhqLEmeUud+9+2JkVRfFldnfOnHN2rh+w4Ozs2Zn7gOR/n33OfZ4nMhNJUnNMVV2AJGlrDG5JahiDW5IaxuCWpIYxuCWpYQxuSWqY0oI7Ij4dEWci4okhjn17RDwaERcj4gOXfe0/RMQT/Y9/XFa9ktQUZY64PwPcM+Sx3wM+BHzu0icj4j3AW4C7gB8BfjEibhxdiZLUPKUFd2Z+DWhf+lxEfH9EfDkiHomI/x0Rb+gf+3RmPg6sX/YydwJfy8yLmXkeeJzhfxhI0q407jnuk8CHM/OHgF8E/tt1jv9j4J6IOBARtwA/ARwruUZJqrWZcb1RRNwA/DjwOxExeHrftb4nMx+KiB8G/g+wBHwDWCuzTkmqu7EFNxuj+7OZeddWvikzfxn4ZYCI+BzwZyXUJkmNMbapksw8B/xVRPxDgNjw5mt9T0RMR8Rr+o/fBLwJeKj0YiWpxqKs1QEj4n7g7wK3AM8B/wb4CvBJ4CiwB/h8Zv67/nTIg8ARYBl4NjP/dkTMAo/2X/Ic8M8z87FSCpakhigtuCVJ5fDOSUlqmFIuTt5yyy15/PjxMl5aknalRx555PnMnBvm2FKC+/jx4ywuLpbx0pK0K0XEXw97rFMlktQwBrckNYzBLUkNY3BLUsMY3JLUMAa3JDWMwS1JDWNwS9IIPPzkc/z6//rLsbyXwS1JI/ClPznNf//G0PfQ7IjBLUkjULS7zB/ZP5b3MrglaQSKTpdjNx8Yy3sZ3JK0Q8urazx37oIjbklqimfO9gA4dsQRtyQ1QtHpB7dTJZLUDEW7C8Cxm50qkaRGKDpd9k5Pceuh2bG8n8EtSTvUave4/ch+pqZiLO9ncEvSDhWd8fVwg8EtSTvW6vSYH1NHCRjckrQj5y9cpH1+ZWwXJmGI4I6IOyLisUs+zkXER8dRnCTVXdHpd5SMccR93V3eM/NPgbsAImIaOAU8WHJdktQIRXu8Pdyw9amSdwJ/mZnjWQJLkmpus4e7xhcn7wXuv9IXIuJERCxGxOLS0tLOK5OkBig6XQ7snebmg3vH9p5DB3dE7AXeB/zOlb6emSczcyEzF+bm5kZVnyTVWtHuMX9kPxHj6eGGrY243w08mpnPlVWMJDVNq9Md64VJ2Fpwf5CrTJNI0iTKTFqd3lgvTMKQwR0RB4F3AV8otxxJao6z3VVeunBxrHdNwhDtgACZeR54Tcm1SFKjbPZw13HELUl6tc0e7hrPcUuSLjEYcc+P8XZ3MLgladtanS437d/DjbN7xvq+BrckbVPR7o11cakBg1uStqmooIcbDG5J2pb19Wp6uMHglqRtWXrpAisX18e6uNSAwS1J2zBYFXDeEbckNcPLGyg44pakRmj1b74Z516TAwa3JG1D0ekyd2gfs3umx/7eBrckbUPR7lUyTQIGtyRtS9HpVtIKCAa3JG3ZxbV1Tr+wXMnNN2BwS9KWnX5hmbX1rOR2dzC4JWnLNlcFdMQtSc3Qqmgd7oFhty47HBEPRMR3IuKpiPixsguTpLoqOl2mAo4enq3k/Yfaugz4NeDLmfmBiNgLVPNjRpJqoGh3OXrTfvZMVzNpcd3gjoibgLcDHwLIzBVgpdyyJKm+ik4163APDPPj4vXAEvCbEfGtiPhUf9f3V4iIExGxGBGLS0tLIy9UkuqiaFezDvfAMME9A7wF+GRm3g2cBz52+UGZeTIzFzJzYW5ubsRlSlI9LK+ucebFC5XdfAPDBXcLaGXmN/ufP8BGkEvSxDl1drC4VI2nSjLzWaCIiDv6T70TeLLUqiSppgbrcFc54h62q+TDwH39jpLvAv+0vJIkqb6KTrU93DBkcGfmY8BCybVIUu212l32zkzxfYf2VVaDd05K0hYUnS7zh/czNRWV1WBwS9IWFO1eJftMXsrglqQtaHW6lXaUgMEtSUN76cJFOt3VSi9MgsEtSUN7uRXQEbckNcJmcDvilqRm2Ozh9uKkJDVD0e5ycO80Rw7sqbQOg1uShtTq7+weUV0PNxjckjS0VqdXeSsgGNySNJTMpGh3K9sg+FIGtyQNodNd5fzKWuUXJsHglqShvNwK6FSJJDVC0al+He4Bg1uShlC069HDDQa3JA2l1ely5MAebtg37P4z5TG4JWkIRadXi44SGHIHnIh4GngRWAMuZqa74UiaKK12lzccPVR1GcDwe04C/ERmPl9aJZJUU+vrSavT41133lp1KYBTJZJ0XWdevMDK2nrlO98MDBvcCTwUEY9ExIkrHRARJyJiMSIWl5aWRlehJFVssxWwBj3cMHxw/53MfAvwbuDnI+Ltlx+QmSczcyEzF+bm5kZapCRV6eUNFBo04s7MU/0/zwAPAm8tsyhJqpNWfx3u2w83ZMQdEQcj4tDgMfD3gSfKLkyS6qJod/m+Q/uY3TNddSnAcF0ltwIP9tefnQE+l5lfLrUqSaqRor8Od11cN7gz87vAm8dQiyTVUtHu8cPHj1RdxibbASXpGlbX1jn9Qq9WI26DW5Ku4fTZZdaz+p3dL2VwS9I1DHq452+uR0cJGNySdE2tzZtvHHFLUiMU7R7TU8HRm2arLmWTwS1J11B0uhy9aZaZ6frEZX0qkaQaKtrdWk2TgMEtSddUdHocq9GFSTC4JemqllfXWHrxgiNuSWqKweJSdbr5BgxuSbqqzR7umqzDPWBwS9JVtGq2DveAwS1JV1F0euydmWLuhn1Vl/IKBrckXUXR7jJ/ZD9TU1F1Ka9gcEvSVRSd+vVwg8EtSVdVtOvXww0GtyRd0bnlVV7orTLf5BF3RExHxLci4otlFiRJddBq93u4mxzcwEeAp8oqRJLqZNDD3dipkoiYB94DfKrcciSpHop2/dbhHhh2xP0J4JeA9asdEBEnImIxIhaXlpZGUpwkVaXV6XHDvhkOH9hTdSmvct3gjoj3Amcy85FrHZeZJzNzITMX5ubmRlagJFVh0MMdUa8ebhhuxP024H0R8TTweeAdEfFbpVYlSRVrdeq1s/ulrhvcmfnxzJzPzOPAvcBXMvOnS69MkiqSmRSdbu0Wlxqwj1uSLtM+v0J3Za2WFyYBZrZycGb+IfCHpVQiSTVR1HQd7gFH3JJ0mc1WwBr2cIPBLUmvsnnzTU2nSgxuSbpM0e5x88G9HNy3pdnksTG4JekyrU6XYzXtKAGDW5JepdXp1XJVwAGDW5Iusb6enOr0mK/phUkwuCXpFZ57cZmVtfXaXpgEg1uSXqFo17uHGwxuSXqFl5dzdapEkhqh1b9r8naDW5Kaoeh0ufXGfeybma66lKsyuCXpEkW7W+sLk2BwS9Ir1Hkd7gGDW5L6VtfWOf1Cr9YXJsHglqRNz5ztsZ4w74hbkpphs4fbOW5JaoZWp97rcA8Ms8v7bET834j444j4dkT823EUJknjVnS6TE8Fr71xtupSrmmYxWYvAO/IzJciYg/w9Yj4/cz8o5Jrk6SxKto9bjs8y8x0vScjrhvcmZnAS/1P9/Q/ssyiJKkKRaf+Pdww5Bx3RExHxGPAGeDhzPxmuWVJ0vgV7d7uCe7MXMvMu4B54K0R8cbLj4mIExGxGBGLS0tLo65TkkrVW1nj+Zcu1P7CJGyxqyQzzwJfBe65wtdOZuZCZi7Mzc2Nqj5JGotTZwcdJbtgxB0RcxFxuP94P/Au4DtlFyZJ4zTo4a7zlmUDw3SVHAU+GxHTbAT9b2fmF8stS5LGq+jUfx3ugWG6Sh4H7h5DLZJUmaLdZd/MFHOH9lVdynXVu1lRksakaPeYP7KfiKi6lOsyuCWJfg93Ay5MgsEtSUAzNlAYMLglTbwXequcW77YiB5uMLglaXNVwCa0AoLBLUmNWYd7wOCWNPGasg73gMEtaeIV7S6H9s1w0/49VZcyFINb0sQrOj3mbz7QiB5uMLgliVan24hb3QcMbkkTLTM31uFuyM03YHBLmnB/c36F3uoa8464JakZivZgVUBH3JLUCEWn38PtVIkkNcNgxO1UiSQ1RKvT5TUH93Jw3zD7ytSDwS1porX6PdxNYnBLmmgby7k2Z5oEhtss+FhEfDUinoyIb0fER8ZRmCSVbW09OXW215hVAQeGmdS5CPxCZj4aEYeARyLi4cx8suTaJKlUz51bZnUtG7O41MB1R9yZeTozH+0/fhF4Cri97MIkqWxN7OGGLc5xR8RxNnZ8/+YVvnYiIhYjYnFpaWk01UlSiZrYww1bCO6IuAH4XeCjmXnu8q9n5snMXMjMhbm5uVHWKEmlaHW6RMBth2erLmVLhgruiNjDRmjfl5lfKLckSRqPot3jtTfOsm9muupStmSYrpIAfgN4KjN/tfySJGk8ik63UXdMDgwz4n4b8DPAOyLisf7HT5ZclySVrtXuNu7CJAzRDpiZXweasS2EJA1p5eI6p88tN+6uSfDOSUkT6pmzPTJp3F2TYHBLmlDF5s7ujrglqRFaDe3hBoNb0oQq2l1mpoLX3tisHm4wuCVNqKLT47bD+5meal7vhcEtaSIV7W7jFpcaMLglTaRWp5k93GBwS5pA3ZWLPP/SSiMvTILBLWkCnep3lDTxdncwuCVNoCb3cIPBLWkCFe1+D7dz3JLUDEW7y+yeKW65YW/VpWyLwS1p4mws53qAjVWrm8fgljRxinavkYtLDRjckiZO0ek29sIkGNySJswLvVVeXL7Y2AuTYHBLmjBFe9AKuIunSiLi0xFxJiKeGEdBklSmVr+He36Xj7g/A9xTch2SNBZN7+GGIYI7M78GtMdQiySVruh0OTQ7w00H9lRdyraNbI47Ik5ExGJELC4tLY3qZSVppIqG7ux+qZEFd2aezMyFzFyYm5sb1ctK0kgVnV6jL0yCXSWSJkhmNnod7gGDW9LEeP6lFZZX1xt98w0M1w54P/AN4I6IaEXEz5VfliSNXrHZCtjsqZKZ6x2QmR8cRyGSVLaXb77Z5SNuSdotWg3f+WbA4JY0MYp2l1tu2MuBvdedbKg1g1vSxGh1eo2+1X3A4JY0MZq+nOuAwS1pIqytJ8+cbfYGCgMGt6SJ8Oy5ZVbX0qkSSWqK3bAO94DBLWkibAa3I25Jaoai0yMCbjvsiFuSGqHV6XL0xln2zjQ/9pp/BpI0hFa7x/wuaAUEg1vShCg63cbf6j5gcEva9S5cXOPZc8u74sIkGNySJsAzZ5fJbP6qgAMGt6Rd7+VWQKdKJKkRBsu5OuKWpIYoOl32TAe33jhbdSkjMVRwR8Q9EfGnEfEXEfGxsouSpFEq2l1uP7yf6amoupSRGGbPyWngvwLvBu4EPhgRd5ZdmCSNSrFL1uEeGGYbiLcCf5GZ3wWIiM8D7weeHHUx/+A/f53l1bVRv6ykCff035znAz80X3UZIzNMcN8OFJd83gJ+5PKDIuIEcALgda973baK+f65g6ysrW/reyXpan7wtYf4RwvHqi5jZEa28VpmngROAiwsLOR2XuMT9949qnIkadca5uLkKeDSH1Xz/eckSRUYJrj/H/ADEfH6iNgL3Av8XrllSZKu5rpTJZl5MSL+BfA/gWng05n57dIrkyRd0VBz3Jn5JeBLJdciSRqCd05KUsMY3JLUMAa3JDWMwS1JDROZ27pX5tovGrEE/PU2v/0W4PkRllOl3XIuu+U8wHOpo91yHrCzc/lbmTk3zIGlBPdORMRiZi5UXcco7JZz2S3nAZ5LHe2W84DxnYtTJZLUMAa3JDVMHYP7ZNUFjNBuOZfdch7gudTRbjkPGNO51G6OW5J0bXUccUuSrsHglqSGqU1w75YNiSPiWER8NSKejIhvR8RHqq5ppyJiOiK+FRFfrLqWnYiIwxHxQER8JyKeiogfq7qm7YiIf9X/t/VERNwfEY3ZujwiPh0RZyLiiUueuzkiHo6IP+//eaTKGod1lXP5j/1/X49HxIMRcbiM965FcO+yDYkvAr+QmXcCPwr8fIPPZeAjwFNVFzECvwZ8OTPfALyZBp5TRNwO/EtgITPfyMZSy/dWW9WWfAa457LnPgb8QWb+APAH/c+b4DO8+lweBt6YmW8C/gz4eBlvXIvg5pINiTNzBRhsSNw4mXk6Mx/tP36RjXC4vdqqti8i5oH3AJ+qupadiIibgLcDvwGQmSuZebbaqrZtBtgfETPAAeCZiusZWmZ+DWhf9vT7gc/2H38W+KmxFrVNVzqXzHwoMy/2P/0jNnYMG7m6BPeVNiRubNgNRMRx4G7gm9VWsiOfAH4JaPouzq8HloDf7E/7fCoiDlZd1FZl5ingPwHfA04DL2TmQ9VWtWO3Zubp/uNngVurLGaE/hnw+2W8cF2Ce9eJiBuA3wU+mpnnqq5nOyLivcCZzHyk6lpGYAZ4C/DJzLwbOE9zfiXf1J//fT8bP4huAw5GxE9XW9Xo5EZ/cuN7lCPiX7MxbXpfGa9fl+DeVRsSR8QeNkL7vsz8QtX17MDbgPdFxNNsTF+9IyJ+q9qStq0FtDJz8NvPA2wEedP8PeCvMnMpM1eBLwA/XnFNO/VcRBwF6P95puJ6diQiPgS8F/gnWdKNMnUJ7l2zIXFEBBvzqE9l5q9WXc9OZObHM3M+M4+z8Xfylcxs5OguM58Fioi4o//UO4EnKyxpu74H/GhEHOj/W3snDbzIepnfA362//hngf9RYS07EhH3sDG1+L7M7Jb1PrUI7v5k/mBD4qeA327whsRvA36GjdHpY/2Pn6y6KAHwYeC+iHgcuAv49xXXs2X93xgeAB4F/oSN/8ONuWU8Iu4HvgHcERGtiPg54FeAd0XEn7PxG8WvVFnjsK5yLv8FOAQ83P+//+ulvLe3vEtSs9RixC1JGp7BLUkNY3BLUsMY3JLUMAa3JDWMwS1JDWNwS1LD/H8rngwpfZHZxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnnModel = RecurrentNeuralNetwork()\n",
    "rnnModel.fit(Xtrain, Ytrain, V, K, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 1569360 values, but the requested shape has 24960\n\t [[node Reshape (defined at <ipython-input-20-56ad4feddb58>:21)  = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](transpose, Reshape/shape)]]\n\nCaused by op 'Reshape', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-1b5cdd9ba8b6>\", line 2, in <module>\n    rnnModel.fit(Xtrain, Ytrain, V, K, epochs=2)\n  File \"<ipython-input-20-56ad4feddb58>\", line 21, in fit\n    outputs = tf.reshape(outputs, (self.sequence_length * batch_size, hidden_layer_size)) # NT x hidden_layer_size\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 6482, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 1569360 values, but the requested shape has 24960\n\t [[node Reshape (defined at <ipython-input-20-56ad4feddb58>:21)  = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](transpose, Reshape/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 1569360 values, but the requested shape has 24960\n\t [[{{node Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](transpose, Reshape/shape)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/notebooks/vendor/machine_learning_examples/nlp_class2/pos_hmm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mYtest\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnnModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# get test acc. too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mrnnModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnnModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mn_test_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_test_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 1569360 values, but the requested shape has 24960\n\t [[node Reshape (defined at <ipython-input-20-56ad4feddb58>:21)  = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](transpose, Reshape/shape)]]\n\nCaused by op 'Reshape', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-1b5cdd9ba8b6>\", line 2, in <module>\n    rnnModel.fit(Xtrain, Ytrain, V, K, epochs=2)\n  File \"<ipython-input-20-56ad4feddb58>\", line 21, in fit\n    outputs = tf.reshape(outputs, (self.sequence_length * batch_size, hidden_layer_size)) # NT x hidden_layer_size\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 6482, in reshape\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 1569360 values, but the requested shape has 24960\n\t [[node Reshape (defined at <ipython-input-20-56ad4feddb58>:21)  = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](transpose, Reshape/shape)]]\n"
     ]
    }
   ],
   "source": [
    "Xtest  = tf.keras.preprocessing.sequence.pad_sequences(Xtest,  maxlen=rnnModel.sequence_length)\n",
    "Ytest  = tf.keras.preprocessing.sequence.pad_sequences(Ytest,  maxlen=rnnModel.sequence_length)\n",
    "# get test acc. too\n",
    "p = rnnModel.sess.run(rnnModel.predict_op, feed_dict={rnnModel.inputs: Xtest, rnnModel.targets: Ytest})\n",
    "n_test_correct = 0\n",
    "n_test_total = 0\n",
    "for yi, pi in zip(Ytest, p):\n",
    "    yii = yi[yi > 0]\n",
    "    pii = pi[yi > 0]\n",
    "    n_test_correct += np.sum(yii == pii)\n",
    "    n_test_total += len(yii)\n",
    "test_acc = float(n_test_correct) / n_test_total\n",
    "\n",
    "print(\n",
    "    \"train acc:\", \"%.4f\" % (float(n_test_correct)/n_test_total),\n",
    "    \"test acc:\", \"%.4f\" % test_acc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models\n",
    "\n",
    "- Hidden Markov Model is an unsupervised model that aims to maximize the probability of whatever given data. \n",
    "  - Observation: $x = \\{x_1, ..., x_t, ..., x_T\\}$\n",
    "  - Maximize: $p(x)$ \n",
    "\n",
    "- There are some latent variables that cause the observations, but they are hidden. Hidden States.\n",
    "    - Hidden States are not required to be known or represent something\n",
    "    - If hidden states are known, it's all a problem of counting p(A | B) = # (A && B) /  # (B)\n",
    "    - If not, use expectation-maximization technique. \n",
    "\n",
    "- Viterbi algorithm\n",
    "    - Reverses the direction of the problem\n",
    "    - Causality: hidden state *causes* the observation\n",
    "    - Viterbi: given a sequence of observation, what is the most likely sequence of hidden states?\n",
    "    - Input: $\\{x_1, ..., x_T\\}$\n",
    "    - Output: $\\{z_1, ..., z_T\\}$\n",
    "\n",
    "- Part-of-speech tagging\n",
    "    - Observations = words\n",
    "    - Hidden states = tags\n",
    "\n",
    "- Probability for Hidden States: P(tag(t) | tag(t-1)) is given by the grammar rules of the language\n",
    "- Probability for Observations: P(word(t) | tag(t)). \n",
    "- Viterbi algorithm uses these two probabilities to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../vendor/machine_learning_examples/'))\n",
    "from hmm_class.hmmd_scaled import HMM\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(T, Y):\n",
    "    n_correct = 0;\n",
    "    n_total = 0;\n",
    "    for t, y in zip(T, Y):\n",
    "        n_correct += np.sum(t == y)\n",
    "        n_total += len(y)\n",
    "    return float(n_correct) / n_total\n",
    "\n",
    "def total_f1_score(T, Y):\n",
    "    T = np.concatenate(T)\n",
    "    Y = np.concatenate(Y)\n",
    "    return f1_score(T, Y, average=None).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, word2idx, tag2idx, unknownIdx = get_data(split_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = 10e-2\n",
    "V = len(word2idx) + 2\n",
    "M = max(max(y) for y in Ytrain) + 1 # number of hidden states\n",
    "A = np.ones((M, M)) * smoothing # P (tag | tag+1)\n",
    "pi = np.zeros(M)                 # P (tag)\n",
    "for y in Ytrain:\n",
    "    pi[y[0]] += 1\n",
    "    for i in range(len(y) - 1):\n",
    "        A[y[i], y[i+1]] += 1\n",
    "A /= A.sum(axis=1, keepdims=True)\n",
    "pi /= pi.sum()\n",
    "\n",
    "B = np.ones((M, V)) * smoothing # P (word | tag)\n",
    "for x, y in zip(Xtrain, Ytrain):\n",
    "    for xi, yi in zip(x, y):\n",
    "        B[yi, xi] += 1\n",
    "B /= B.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(M)\n",
    "hmm.pi = pi\n",
    "hmm.A = A\n",
    "hmm.B = B\n",
    "\n",
    "Ptrain = []\n",
    "for x in Xtrain:\n",
    "    p = hmm.get_state_sequence(x)\n",
    "    Ptrain.append(p)\n",
    "\n",
    "Ptest = []\n",
    "for x in Xtest:\n",
    "    p = hmm.get_state_sequence(x)\n",
    "    Ptest.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.9634104294681358\n",
      "test accuracy:  0.9179981847732022\n",
      "train f1 score:  0.9032321066180082\n",
      "test f1 score:  0.8398427947445825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy: \", accuracy(Ytrain, Ptrain))\n",
    "print(\"test accuracy: \", accuracy(Ytest, Ptest))\n",
    "print(\"train f1 score: \", total_f1_score(Ytrain, Ptrain))\n",
    "print(\"test f1 score: \", total_f1_score(Ytest, Ptest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9737539378539345\n",
      "test accuracy: 0.9287840091183486\n",
      "train f1: 0.9235279546052866\n",
      "test f1: 0.862609207175332\n"
     ]
    }
   ],
   "source": [
    "%run -i '../vendor/machine_learning_examples/nlp_class2/pos_hmm.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "- Similar to POS tagging: assign a tag to each token in a sentence.\n",
    "- Named entities: person, company, location, etc. \n",
    "- NER is very imbalanced (~90% is not an entity!)\n",
    "\n",
    "Data in: https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/ner.txt\n",
    "\n",
    "## Comparison with POS\n",
    "\n",
    "- It's all about the input format of data. \n",
    "- NER is the same as POS in terms of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 46469\n",
      "number of classes: 21\n",
      "vocabulary size: 9068\n",
      "epoch: 0\n",
      "i: 0 j: 0 n_batches: 325 cost: 3.042160304278263 error: 0.98\n",
      "i: 0 j: 200 n_batches: 325 cost: 0.6731696576582723 error: 0.07\n",
      "epoch: 1\n",
      "i: 1 j: 0 n_batches: 325 cost: 0.36689083040952447 error: 0.04\n",
      "i: 1 j: 200 n_batches: 325 cost: 0.12609729498124853 error: 0.02\n",
      "epoch: 2\n",
      "i: 2 j: 0 n_batches: 325 cost: 0.3950151150887439 error: 0.08\n",
      "i: 2 j: 200 n_batches: 325 cost: 0.3085001759571853 error: 0.05\n",
      "epoch: 3\n",
      "i: 3 j: 0 n_batches: 325 cost: 0.22364010638076845 error: 0.04\n",
      "i: 3 j: 200 n_batches: 325 cost: 0.30238850848261495 error: 0.05\n",
      "epoch: 4\n",
      "i: 4 j: 0 n_batches: 325 cost: 0.16972745431690509 error: 0.03\n",
      "i: 4 j: 200 n_batches: 325 cost: 0.3338048592223948 error: 0.06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "train score: 0.9504442190045805\n",
      "train f1 score: 0.08706716092292759\n",
      "test score: 0.9479196556671449\n",
      "test f1 score: 0.08335426288735479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%run -i '../vendor/machine_learning_examples/nlp_class2/ner_baseline.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 2394\n",
      "number of classes: 21\n",
      "Xtrain.shape: (1676, 39)\n",
      "Ytrain.shape: (1676, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 45.8911 train acc: 0.7940 test acc: 0.9389 time for epoch: 0:00:01.433327\n",
      "i: 1 cost: 9.3078 train acc: 0.9476 test acc: 0.9417 time for epoch: 0:00:00.690508\n",
      "i: 2 cost: 8.5426 train acc: 0.9487 test acc: 0.9420 time for epoch: 0:00:00.679406\n",
      "i: 3 cost: 8.1259 train acc: 0.9487 test acc: 0.9421 time for epoch: 0:00:00.670533\n",
      "i: 4 cost: 7.6168 train acc: 0.9490 test acc: 0.9422 time for epoch: 0:00:00.655762\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i '../vendor/machine_learning_examples/nlp_class2/ner_tf.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work\n",
    "\n",
    "- NER and POS examples worked well with the baseline model.\n",
    "- More complex models tend to overfit the data. \n",
    "- Find the sweet spot with good accuracy on train and test sets.\n",
    "- Ask question, is error tolerable? Then keep the model simple.\n",
    "- Change hyperparameters of RNN to get better results than Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
