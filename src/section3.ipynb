{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "\n",
    "## Review of Language Modeling and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language model using bigrams - `p(current_word | previous_word)`. This is a Markov model, a probabilistic model in which predictions are made based only on current state.\n",
    "- Model previous language in a neural version. Logistic regression.\n",
    "- Neural network of the previous model.\n",
    "- Improve efficiency of the above with an implementation trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and language models\n",
    "\n",
    "- Language model: a model of the probability of a sequence of words\n",
    "  - given a sentence `s`, the language model gives `p(s)`\n",
    "  - creating the model typically involve making assumptions about the structure of a specific language\n",
    "- **Note:** A model is never 100% correct, it has assumptions, which may be:\n",
    "  - Correct most of the times, incorrect sometimes\n",
    "  - Incorrect most of the times, but still powerfull. \n",
    "- Example: a map. \"The map is not the territory\"\n",
    "\n",
    "- Bigram: two consecutive words in a sentence\n",
    "- Trigrams: three consecutive words in a sentence\n",
    "- N-grams: sequence of `n` consecutive words\n",
    "- Bigram model: `p(w_t | w_t-1)`\n",
    "    - given a set of documents\n",
    "    - build the model by counting: # appearances `w_t-1 w_t` / # appearances `w_t-1`\n",
    "- Set of documents: it a file of files containing sentences. Training corpus. \n",
    "\n",
    "- Using Bigram models we'll build a language model: \n",
    "    - Bayes rule: `p(A -> B -> C) = p(C|A -> B) * p(A -> B) = p(C|A -> B) * p(B|A) * p(A)`\n",
    "    - Chain rule of probability\n",
    "    - `p(B|A)` is a Bigram model\n",
    "    - `p(C|A -> B) = count(A -> B -> C) / count(A -> B)`\n",
    "    - `p(A) = count(A) / corpus length`\n",
    "    - For long sentences this becomes problematic\n",
    "    \n",
    "- Long sentences, say `\"A B C D E F\"`, may lead to `p(G| A B C D E) = 0` which may not be true. \n",
    "    - Add-one smoothing: `p_smooth(B|A) = (count(A -> B) + 1) / (count(A) + V)`\n",
    "    - V: vocabulary size = number of distinct words\n",
    "    - Adding V to the denominator makes the probability valid, i.e. summing all values gives 1.\n",
    "    - Also this smoothing allows for not having a probability of 0 for any pair A and B. \n",
    "    \n",
    "- Markov Assumption\n",
    "    - What I see now depends *only* on what I saw in the previous step.\n",
    "    - `p(w_t | w_t-1, w_t-2, ..., w_1) = p(w_t | w_t-1)`\n",
    "    - Second, third, ... order Markov. \n",
    "    - `p(A B C D E) = p(E | D) * p(D | C) * p(C | B) * p(B | A) * p(A)`\n",
    "    - All elements are bigrams!\n",
    "\n",
    "- Long sentences are rare, but small sentences, like bigrams, may be really common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Brown corpus using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents()\n",
    "print('\\n\\n'.join(' '.join(s) for s in sentences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bigram language model with add-one smoothing. \n",
    "- Use lower case preprocessing.\n",
    "\n",
    "Hints: \n",
    "- Use log probabilities to avoid underflow to 0\n",
    "- Normalize each sentence, by dividing by their length $T$\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} log p(w_1, ..., w_T) = \\frac{1}{T} \\left[ log p(w_1) + \\sum^{T}_{t=2} log p (w_t | w_t-1) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model\n",
    "- Compare the probability of a real sentence from the corpus vs. a fake sentence (randomly generated words)\n",
    "- Compare a fake sentence vs. a custom valid sentence written by me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
