{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "\n",
    "## Review of Language Modeling and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language model using bigrams - `p(current_word | previous_word)`. This is a Markov model, a probabilistic model in which predictions are made based only on current state.\n",
    "- Model previous language in a neural version. Logistic regression.\n",
    "- Neural network of the previous model.\n",
    "- Improve efficiency of the above with an implementation trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and language models\n",
    "\n",
    "- Language model: a model of the probability of a sequence of words\n",
    "  - given a sentence `s`, the language model gives `p(s)`\n",
    "  - creating the model typically involve making assumptions about the structure of a specific language\n",
    "- **Note:** A model is never 100% correct, it has assumptions, which may be:\n",
    "  - Correct most of the times, incorrect sometimes\n",
    "  - Incorrect most of the times, but still powerfull. \n",
    "- Example: a map. \"The map is not the territory\"\n",
    "\n",
    "- Bigram: two consecutive words in a sentence\n",
    "- Trigrams: three consecutive words in a sentence\n",
    "- N-grams: sequence of `n` consecutive words\n",
    "- Bigram model: `p(w_t | w_t-1)`\n",
    "    - given a set of documents\n",
    "    - build the model by counting: # appearances `w_t-1 w_t` / # appearances `w_t-1`\n",
    "- Set of documents: it a file of files containing sentences. Training corpus. \n",
    "\n",
    "- Using Bigram models we'll build a language model: \n",
    "    - Bayes rule: `p(A -> B -> C) = p(C|A -> B) * p(A -> B) = p(C|A -> B) * p(B|A) * p(A)`\n",
    "    - Chain rule of probability\n",
    "    - `p(B|A)` is a Bigram model\n",
    "    - `p(C|A -> B) = count(A -> B -> C) / count(A -> B)`\n",
    "    - `p(A) = count(A) / corpus length`\n",
    "    - For long sentences this becomes problematic\n",
    "    \n",
    "- Long sentences, say `\"A B C D E F\"`, may lead to `p(G| A B C D E) = 0` which may not be true. \n",
    "    - Add-one smoothing: `p_smooth(B|A) = (count(A -> B) + 1) / (count(A) + V)`\n",
    "    - V: vocabulary size = number of distinct words\n",
    "    - Adding V to the denominator makes the probability valid, i.e. summing all values gives 1.\n",
    "    - Also this smoothing allows for not having a probability of 0 for any pair A and B. \n",
    "    \n",
    "- Markov Assumption\n",
    "    - What I see now depends *only* on what I saw in the previous step.\n",
    "    - `p(w_t | w_t-1, w_t-2, ..., w_1) = p(w_t | w_t-1)`\n",
    "    - Second, third, ... order Markov. \n",
    "    - `p(A B C D E) = p(E | D) * p(D | C) * p(C | B) * p(B | A) * p(A)`\n",
    "    - All elements are bigrams!\n",
    "\n",
    "- Long sentences are rare, but small sentences, like bigrams, may be really common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Brown corpus using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents()\n",
    "print('\\n\\n'.join(' '.join(s) for s in sentences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bigram language model with add-one smoothing. \n",
    "- Use lower case preprocessing.\n",
    "\n",
    "Hints: \n",
    "- Use log probabilities to avoid underflow to 0\n",
    "- Normalize each sentence, by dividing by their length $T$\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} log p(w_1, ..., w_T) = \\frac{1}{T} \\left[ log p(w_1) + \\sum^{T}_{t=2} log p (w_t | w_t-1) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovBigramLanguageModel:\n",
    "    def __init__(self, language, smooth = True):\n",
    "        try:\n",
    "            self.stopwords = stopwords.words(language)\n",
    "        except:\n",
    "            self.stopwords = []\n",
    "        self.smooth = smooth\n",
    "        self.wordCount = {}\n",
    "        self.wordProb = {}\n",
    "        self.bigramCount = {}\n",
    "        self.bigramProb = {}\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            normSentence = self._preprocessSentence(sentence)\n",
    "            if len(normSentence) == 0: \n",
    "                continue\n",
    "            self._addWordCount(normSentence[0])\n",
    "            for i in range(1, len(normSentence)):\n",
    "                prevWord = normSentence[i-1]\n",
    "                curWord = normSentence[i]\n",
    "                self._addWordCount(curWord)\n",
    "                self._addBigramCount(prevWord, curWord)\n",
    "        self._calcProbabilityMatrices()\n",
    "    \n",
    "    def probability(self, sentence):\n",
    "        normSentence = self._preprocessSentence(sentence)\n",
    "        if len(normSentence) == 0:\n",
    "            return 0.0\n",
    "        prob = self.wordProb[normSentence[0]]\n",
    "        for i in range(1, len(normSentence)):\n",
    "            prevWord = normSentence[i-1]\n",
    "            curWord = normSentence[i]\n",
    "            if prevWord in self.bigramProb and curWord in self.bigramProb[prevWord]:\n",
    "                prob += self.bigramProb[prevWord][curWord]\n",
    "        return prob / len(normSentence)\n",
    "    \n",
    "    def _calcProbabilityMatrices(self):\n",
    "        vocabSize = float(len(self.wordCount))\n",
    "        for w in self.wordCount:\n",
    "            self.wordProb[w] = self.wordCount[w] / vocabSize\n",
    "        for w0 in self.bigramCount:\n",
    "            self.bigramProb[w0] = {}\n",
    "            for w1 in self.bigramCount[w0]:\n",
    "                if (self.smooth):\n",
    "                    bp = (self.bigramCount[w0][w1] + 1.0)/ (self.wordCount[w0] + vocabSize)\n",
    "                else:\n",
    "                    bp = self.bigramCount[w0][w1] / self.wordCount[w0]\n",
    "                self.bigramProb[w0][w1] = bp\n",
    "        \n",
    "    def _addWordCount(self, inputWord):\n",
    "        if inputWord not in self.wordCount:\n",
    "            self.wordCount[inputWord] = 0.0\n",
    "        self.wordCount[inputWord] += 1.0\n",
    "\n",
    "    def _addBigramCount(self, prevWord, curWord):\n",
    "        if prevWord not in self.bigramCount:\n",
    "            self.bigramCount[prevWord] = {}\n",
    "        if curWord not in self.bigramCount[prevWord]:\n",
    "            self.bigramCount[prevWord][curWord] = 0.0\n",
    "        self.bigramCount[prevWord][curWord] += 1.0\n",
    "        \n",
    "    def _preprocessSentence(self, sentence):\n",
    "        preprocessedWords = [self._preprocessWord(w) for w in sentence]\n",
    "        normSentence = []\n",
    "        for w in preprocessedWords:\n",
    "            if w not in self.stopwords:\n",
    "                normSentence.append(w)\n",
    "        return normSentence\n",
    "\n",
    "    def _preprocessWord(self, word):\n",
    "        return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model\n",
    "- Compare the probability of a real sentence from the corpus vs. a fake sentence (randomly generated words)\n",
    "- Compare a fake sentence vs. a custom valid sentence written by me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generateTestSentences():\n",
    "    words = list(brown.words())\n",
    "    sentences = list(brown.sents())\n",
    "    random.shuffle(words)\n",
    "    corpusSentence = random.choice(sentences)\n",
    "    fakeSentence = words[:10]\n",
    "    realSentence = \"I ride my bike on the mountain\".split(' ')\n",
    "    return {\n",
    "        'corpus': corpusSentence,\n",
    "        'fake': fakeSentence,\n",
    "        'real': realSentence,\n",
    "    }\n",
    "\n",
    "def testModel(model, testSentences):\n",
    "    report = {}\n",
    "    for t in testSentences: \n",
    "        sentence = testSentences[t]\n",
    "        prob = model.probability(sentence)\n",
    "        report[t] = (prob, ' '.join(sentence))\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = brown.sents()\n",
    "testSentences = generateTestSentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MarkovBigramLanguageModel(language='english', smooth=True)\n",
    "model1.train(trainSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MarkovBigramLanguageModel(language='english', smooth=False)\n",
    "model2.train(trainSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MarkovBigramLanguageModel(language='', smooth=True)\n",
    "model3.train(trainSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MarkovBigramLanguageModel(language='', smooth=False)\n",
    "model4.train(trainSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportStopWordsSmooth = testModel(model1, testSentences)\n",
    "reportStopWordsNoSmooth = testModel(model2, testSentences)\n",
    "reportAllWordsSmooth = testModel(model3, testSentences)\n",
    "reportAllWordsNoSmooth = testModel(model4, testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': (0.002133892069761611,\n",
       "  u'They are still considered to be for use in restricted waters , however , and targets must come within a few yards of them .'),\n",
       " 'fake': (0.0002711037433867193,\n",
       "  u'witnesses board despise Administration both was Give his , .'),\n",
       " 'real': (0.00032893632732521066, 'I ride my bike on the mountain')}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reportStopWordsSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': (0.10022049539158925,\n",
       "  u'They are still considered to be for use in restricted waters , however , and targets must come within a few yards of them .'),\n",
       " 'fake': (0.002976413139989482,\n",
       "  u'witnesses board despise Administration both was Give his , .'),\n",
       " 'real': (0.00032893632732521066, 'I ride my bike on the mountain')}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reportStopWordsNoSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': (0.007645759363363574,\n",
       "  u'They are still considered to be for use in restricted waters , however , and targets must come within a few yards of them .'),\n",
       " 'fake': (7.529510808173558e-05,\n",
       "  u'witnesses board despise Administration both was Give his , .'),\n",
       " 'real': (0.021063662621245625, 'I ride my bike on the mountain')}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reportAllWordsSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus': (0.08072599697821253,\n",
       "  u'They are still considered to be for use in restricted waters , however , and targets must come within a few yards of them .'),\n",
       " 'fake': (0.0017091119743144491,\n",
       "  u'witnesses board despise Administration both was Give his , .'),\n",
       " 'real': (0.06712548302474791, 'I ride my bike on the mountain')}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reportAllWordsNoSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
