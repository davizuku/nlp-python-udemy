{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Neural Networks\n",
    "\n",
    "Sentence representation\n",
    "- Bag of words\n",
    "    - order is lost\n",
    "- Recurrent Neural Networks\n",
    "    - sequences of words\n",
    "    - hard to process long sentences\n",
    "- Tree!\n",
    "    - What does the input data look like?\n",
    "    - How to transform it into a tree?\n",
    "    \n",
    "Recursive Networks\n",
    "- Plain recursive network\n",
    "$$Linear: h' = f(W^Th + b)$$\n",
    "- Recursive Neural Tensor Network\n",
    "$$Quadratic: h' = f(h^T Ah + W^T h + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences as trees\n",
    "\n",
    "- Consider part-of-speech tags (N, V, D, ...)\n",
    "- Sentences are hierarchycal, subject, verb, verb phrase, noun phrase, etc. \n",
    "- Sentiment analysis\n",
    "    - Each node in the tree will have a label to indicate sentiment\n",
    "    - This helps with negation problem\n",
    "\n",
    "- Building a tree is also a ML problem. \n",
    "- We'll work with binary trees, although trees can have multiple children. \n",
    "    - Always 2 children or 0, never single child.\n",
    "\n",
    "- Sentiment Analysis\n",
    "    - Each node has a label for sentiment\n",
    "    - Only leaves of the tree are actual words\n",
    "\n",
    "Data: https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Neural Networks\n",
    "\n",
    "- Tree Neural Networks (TNN)\n",
    "- For each leaf node with a word, let $w_i$ be the word embedding for that word.\n",
    "\n",
    "### Binary trees\n",
    "- For each inner node we define: $h_i = f(W_{left} x_{left} + W_{right} x_{right} + b)$\n",
    "    - Let $D$ be the embedding size of words.\n",
    "    - Weight matrices $W$ should be of size $D x D$.\n",
    "    - Biases $b$ should be of size $D$.\n",
    "    - $f$ = relu, tanh, etc. \n",
    "\n",
    "- Given a label $y$ and an inner/root node $h$, calculating $p(y | h) = softmax(W_o h + b_o)$ \n",
    "    - $W_o$ weight matrix\n",
    "    - $b_o$ bias term\n",
    "\n",
    "### N-ary trees\n",
    "\n",
    "- For each inner node we define: $h = f(\\sum_i W(rel(h, i)) x_i + b)$\n",
    "    - Let $D$ be the embedding size of words.\n",
    "    - Weight matrices $W$ should be of size $R x D x D$.\n",
    "    - $R$ is the number of possible parent-child relationships\n",
    "    - $rel(h, i) -> \\{1, ..., R\\}$ tells the type of of relationship\n",
    "    - Biases $b$ should be of size $D$.\n",
    "    - $f$ = relu, tanh, etc. \n",
    "\n",
    "- Given a label $y$ and an inner/root node $h$, calculating $p(y | h) = softmax(W_o h + b_o)$ \n",
    "    - $W_o$ weight matrix\n",
    "    - $b_o$ bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../vendor/machine_learning_examples/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ops\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "%run -i '../vendor/machine_learning_examples/nlp_class2/recursive_tensorflow.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This solution is not idea.\n",
    "- Each sentence is a different graph, different costs, different gradients, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees to Sequences\n",
    "\n",
    "- TNNs - trees\n",
    "- Recurrent NNs - sequences\n",
    "- Convert tree to sequence\n",
    "\n",
    "- Convert the tree into two arrays: \n",
    "    - Elements array: postorder traversal of nodes\n",
    "    - Parents array: for each node at the position `i` in the elements array, parents array in the position `i` contains the id of its parent. \n",
    "    - For N-ary trees, create Relations array: where each position `i` contains the type of relation of node `i` with its parent.\n",
    "    - Word array: each position `i` contains a word if node `i` is a leaf of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 18647\n",
      "i: 0 cost: 5018.125420100336 correct rate: 0.5218208092485549 time for epoch: 0:08:00.790750\n",
      "i: 1 cost: 4935.90714544377 correct rate: 0.522543352601156 time for epoch: 0:08:53.519088\n",
      "i: 2 cost: 4887.338247981783 correct rate: 0.5296242774566474 time for epoch: 0:08:54.141769\n",
      "i: 3 cost: 4855.987570344421 correct rate: 0.5382947976878613 time for epoch: 0:11:28.104376\n",
      "i: 4 cost: 4834.864995274912 correct rate: 0.5449421965317919 time for epoch: 0:09:01.347198\n",
      "i: 5 cost: 4819.428320187071 correct rate: 0.5491329479768786 time for epoch: 0:08:50.154939\n",
      "i: 6 cost: 4807.048681661282 correct rate: 0.557514450867052 time for epoch: 0:07:48.398398\n",
      "i: 7 cost: 4794.498076083394 correct rate: 0.5635838150289018 time for epoch: 0:06:57.857527\n",
      "i: 8 cost: 4780.833737315014 correct rate: 0.5686416184971098 time for epoch: 0:06:59.001770\n",
      "i: 9 cost: 4760.658218977364 correct rate: 0.5878612716763005 time for epoch: 0:06:58.997463\n",
      "i: 10 cost: 4670.462795350324 correct rate: 0.6232658959537573 time for epoch: 0:06:59.166411\n",
      "i: 11 cost: 4096.80460686585 correct rate: 0.7199421965317919 time for epoch: 0:06:59.613346\n",
      "i: 12 cost: 3077.550129121018 correct rate: 0.8419075144508671 time for epoch: 0:06:59.530583\n",
      "i: 13 cost: 2231.880444217245 correct rate: 0.9102601156069364 time for epoch: 0:07:04.161359\n",
      "i: 14 cost: 1764.460189925225 correct rate: 0.9446531791907514 time for epoch: 0:07:05.362836\n",
      "i: 15 cost: 1446.430140908914 correct rate: 0.9614161849710983 time for epoch: 0:07:05.706585\n",
      "i: 16 cost: 1282.6639688238033 correct rate: 0.9715317919075145 time for epoch: 0:07:14.268794\n",
      "i: 17 cost: 1133.6524627833185 correct rate: 0.9783236994219653 time for epoch: 0:07:12.485370\n",
      "i: 18 cost: 1065.8005006355 correct rate: 0.9791907514450867 time for epoch: 0:07:07.475937\n",
      "i: 19 cost: 1151.4891442679047 correct rate: 0.9776011560693642 time for epoch: 0:07:08.315134\n",
      "n_correct: 6816 n_total: 6920 train accuracy: 0.9849710982658959\n",
      "n_correct: 756 n_total: 1000 test accuracy: 0.756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i '../vendor/machine_learning_examples/nlp_class2/recursive_theano.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Neural Tensor Networks\n",
    "\n",
    "- RNTN\n",
    "- Assumes working with Binary Tree\n",
    "- Concatenate together the inputs form children $x_{left}$ and $x_{right}$. \n",
    "    - $h = f(W^Tx + b)$\n",
    "    - $x$ now is of size $2D$\n",
    "    - $W$ of size $2D x D$\n",
    "- Adding a quadratic element, the inner node formula is for an index $j$: \n",
    "    - $h_j = f(x^T A_j x + W_j^T x + b_j)$ \n",
    "        - Each element is an scalar. \n",
    "    - $A_j$ is of size $2D x 2D$.\n",
    "    - Since $j = 1..D$, then $A$ is of size $D x 2D x 2D$\n",
    "- $h(x) = x^T A x + B^T x + c$, quadratic discriminant analysis (2 Gaussians with different covariance)\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "- Recursive neural network was: \n",
    "$$h(x_L, x_R) = f(W_L^T x_L + W_R^T x_R + b)$$\n",
    "- Adding quadratic weights: \n",
    "$$h(x_L, x_R) = f(x_L^T A_{LL} x_L + x_L^T A_{LR} x_R + x_R^T A_{RR} x_R + W_L^T x_L + W_R^T x_R + b$$\n",
    "- No more need of relations and parents arrays\n",
    "- Three lists instead (filled with -1 if null): \n",
    "    - words\n",
    "    - left_children\n",
    "    - right_children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNTN Tensorflow tips\n",
    "\n",
    "- A custom solution is needed for RNTNs, RNN cells like LSTM or GRU doesn't work. \n",
    "- Four sequences to loop simultaneously: \n",
    "    - words: contains the word for the current node\n",
    "    - left_children: contains the index for the left child\n",
    "    - right_children: contains the index for the right child\n",
    "    - labels: contains the sentiment for the current node\n",
    "\n",
    "- Iterating throught these sequences depend on the actual data that we haven't provided yet. So TF has a useful function `while_loop` to allow defining the graph structure iterating over future data. \n",
    "\n",
    "```\n",
    "outputs = tf.while_loop(\n",
    "    condition, \n",
    "    body,  # set state, return output\n",
    "    initial_values\n",
    ")\n",
    "```\n",
    "\n",
    "- Condition: \n",
    "    - `tf.less(n, tf.shape(words)[0])`\n",
    "    \n",
    "- Body (only binary tree): \n",
    "    - (pseudocode)\n",
    "    ```\n",
    "    if current node is leaf: \n",
    "        hidden value = embedding[current word]\n",
    "    else: \n",
    "        hidden value = tanh(h_l.dot(W_l) + h_r.dot(W_r) + b)\n",
    "    ```\n",
    "    - (tensorflow)\n",
    "    ```\n",
    "    h_n = tf.cond(\n",
    "        current_word >= 0, # is leaf?\n",
    "        lambda: embedding[current_word],\n",
    "        lambda: <recursive_calculation>\n",
    "    )\n",
    "    ```\n",
    "- Update hidden nodes: \n",
    "    ```\n",
    "    hiddens = hiddens.write(n, h_n) # hiddens[n] = h_n\n",
    "    ```\n",
    "    - What is `hiddens` a `placeholder` or `Variable`?\n",
    "        - Neither, it is an intermediate output. \n",
    "        - It is a `TensorArray`: https://www.tensorflow.org/api_docs/python/tf/TensorArray\n",
    "            - `read()`\n",
    "            - `write()`\n",
    " \n",
    "\n",
    "#### What to train on?\n",
    "\n",
    "Two options: \n",
    "- Only label at root node\n",
    "```\n",
    "logits = tf.matmul(hiddens.stack(), Wo) + bo\n",
    "cost_op = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits[-1],       # -1 references to root node (due to postorder)\n",
    "        labels=labels[-1]\n",
    "    )\n",
    ")\n",
    "```\n",
    "- Labels at all nodes\n",
    "    - Sentiment from 0..4 bucketed into: \n",
    "        - Positive: 3,4\n",
    "        - Neutral: 2\n",
    "        - Negative: 0,1\n",
    "    - Remove neutral sentences\n",
    "        - Also from inner nodes\n",
    "    ```\n",
    "    labeled_indices = tf.where(labels >= 0)\n",
    "    cost_op = tf.reduce_mean(...\n",
    "        logits = tf.gather(logits, labeled_indices),\n",
    "        labels = tf.gather(labels, labeled_indices), ... )    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
